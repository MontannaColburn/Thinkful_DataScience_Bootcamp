{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Learning Capstone",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Hkky6yu17m7K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Unit 4 Unsupervised Learning Capstone Project"
      ]
    },
    {
      "metadata": {
        "id": "q-YJFoxB7oq7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
        "\n",
        "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
        "\n",
        "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
        "\n",
        "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
        "\n",
        "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
        "\n",
        "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
        "\n",
        "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
        "\n",
        "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
      ]
    },
    {
      "metadata": {
        "id": "HvmEOqWjUPXb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dataset: https://www.kaggle.com/snap/amazon-fine-food-reviews "
      ]
    },
    {
      "metadata": {
        "id": "TYaYUef17foi",
        "colab_type": "code",
        "outputId": "c2b34da7-1f84-4aeb-93dd-8943a79937a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "#Basic imports \n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "#NLP imports \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Dimension Reduction \n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "#Clustering \n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "#Clustering Evaluation \n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "#Model Imports  \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "#Model Optimization \n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Time\n",
        "import time "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2HaUp9cz7geq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://www.dropbox.com/s/d4ye48a67tth2ae/Reviews.csv?dl=1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AHBeCzvK7kxF",
        "colab_type": "code",
        "outputId": "253abfc0-4c39-40fe-cbe6-0639780af36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "#drop uneccessary columns \n",
        "df.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Time', 'Summary'],axis=1,inplace=True)\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Score</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Score                                               Text\n",
              "0      5  I have bought several of the Vitality canned d...\n",
              "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2      4  This is a confection that has been around a fe...\n",
              "3      2  If you are looking for the secret ingredient i...\n",
              "4      5  Great taffy at a great price.  There was a wid..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "-CghfuXu7s-c",
        "colab_type": "code",
        "outputId": "aaf1368c-4e16-4c31-ab8e-33bc3200b94a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 568454 entries, 0 to 568453\n",
            "Data columns (total 2 columns):\n",
            "Score    568454 non-null int64\n",
            "Text     568454 non-null object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 8.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZAafWSTa7uL0",
        "colab_type": "code",
        "outputId": "ad1e8653-2283-4502-bf87-64081282f1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Score    0\n",
              "Text     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "Qe7G30GI7vw6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove duplicate reviews  \n",
        "df.drop_duplicates(subset=['Score','Text'],keep='first',inplace=True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AB7BAtSc7xJ1",
        "colab_type": "code",
        "outputId": "b9db229e-a171-4ee5-acb1-36eccfbe6423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "df.Score.value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    250745\n",
              "4     56074\n",
              "1     36280\n",
              "3     29772\n",
              "2     20804\n",
              "Name: Score, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "yH4q9tSF7yZK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to clean text.\n",
        "def text_cleaner(text):\n",
        "    \n",
        "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
        "    \n",
        "    text = re.sub(r'--',' ',text)\n",
        "    \n",
        "    # Removes hyperlinks \n",
        "    text = re.sub(r'<a\\s+href=(?:\"([^\"]+)\"|\\'([^\\']+)\\').*?>(.*?)</a>',' ', text)\n",
        "    \n",
        "    # Get rid of extra whitespace.\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    # Lowercase text\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4jncnFX970Ei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['Clean'] = df['Text'].apply(lambda x: text_cleaner(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-kKOWCjZ71ec",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create Utility function to lemmatize our text reviews limiting variations on same words\n",
        "lemma = spacy.lang.en.English()\n",
        "\n",
        "def lemma_text(text):\n",
        "    tokens = lemma(text) \n",
        "    return([token.lemma_ for token in tokens if not token.is_punct and not token.is_stop])\n",
        "\n",
        "df['lemma_text'] = df.Clean.apply(lemma_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EHkWHtko73PA",
        "colab_type": "code",
        "outputId": "a96b4b54-8e98-4dcd-d270-c9555be1d07c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Score</th>\n",
              "      <th>Text</th>\n",
              "      <th>Clean</th>\n",
              "      <th>lemma_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>[buy, vitality, can, dog, food, product, find,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>[product, arrive, label, jumbo, salt, peanut, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>[confection, century, light, pillowy, citrus, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>[look, secret, ingredient, robitussin, believe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>great taffy at a great price. there was a wide...</td>\n",
              "      <td>[great, taffy, great, price, wide, assortment,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Score                                               Text  \\\n",
              "0      5  I have bought several of the Vitality canned d...   \n",
              "1      1  Product arrived labeled as Jumbo Salted Peanut...   \n",
              "2      4  This is a confection that has been around a fe...   \n",
              "3      2  If you are looking for the secret ingredient i...   \n",
              "4      5  Great taffy at a great price.  There was a wid...   \n",
              "\n",
              "                                               Clean  \\\n",
              "0  i have bought several of the vitality canned d...   \n",
              "1  product arrived labeled as jumbo salted peanut...   \n",
              "2  this is a confection that has been around a fe...   \n",
              "3  if you are looking for the secret ingredient i...   \n",
              "4  great taffy at a great price. there was a wide...   \n",
              "\n",
              "                                          lemma_text  \n",
              "0  [buy, vitality, can, dog, food, product, find,...  \n",
              "1  [product, arrive, label, jumbo, salt, peanut, ...  \n",
              "2  [confection, century, light, pillowy, citrus, ...  \n",
              "3  [look, secret, ingredient, robitussin, believe...  \n",
              "4  [great, taffy, great, price, wide, assortment,...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "y6mhzWVJ77Vo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature Generation"
      ]
    },
    {
      "metadata": {
        "id": "gHMpdudV75LB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Identifying variables\n",
        "X = df['Clean']\n",
        "y = df['Score']\n",
        "\n",
        "# Splitting into train and test sets, reserve 40% for test \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vrcTxD379OP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Total word count \n",
        "feature_df = pd.DataFrame()\n",
        "feature_df['word_count'] = [len(x.split()) for x in X_train.tolist()]\n",
        "\n",
        "# Count of punctuations \n",
        "feature_df['exclamation_marks'] = X_train.str.findall(r'[!]').str.len()\n",
        "feature_df['periods'] = X_train.str.findall(r'[.]').str.len()\n",
        "feature_df['question_marks'] = X_train.str.findall(r'[?]').str.len()\n",
        "feature_df['Text'] = X_train\n",
        "feature_df['Score'] = y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z29WX3Cf8B67",
        "colab_type": "code",
        "outputId": "2cc0012e-24e4-4121-cd13-12eed576046f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "# Let's check our new features\n",
        "feature_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_count</th>\n",
              "      <th>exclamation_marks</th>\n",
              "      <th>periods</th>\n",
              "      <th>question_marks</th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>204</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>175</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>great taffy at a great price. there was a wide...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_count  exclamation_marks  periods  question_marks  \\\n",
              "0          64                0.0      3.0             0.0   \n",
              "1         204                0.0      5.0             0.0   \n",
              "2         175                0.0      9.0             0.0   \n",
              "3          86                0.0      3.0             0.0   \n",
              "4          60                0.0      4.0             0.0   \n",
              "\n",
              "                                                Text  Score  \n",
              "0  i have bought several of the vitality canned d...    5.0  \n",
              "1  product arrived labeled as jumbo salted peanut...    1.0  \n",
              "2  this is a confection that has been around a fe...    4.0  \n",
              "3  if you are looking for the secret ingredient i...    2.0  \n",
              "4  great taffy at a great price. there was a wide...    5.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "0M8sJASp8GRT",
        "colab_type": "code",
        "outputId": "124c72f5-0e6c-4565-bbb6-a3d3ec0effd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialize vectorizer \n",
        "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
        "                             min_df=2, # only use words that appear at least twice\n",
        "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
        "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
        "                             smooth_idf=True, #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
        "                             tokenizer=lemma_text)\n",
        "\n",
        "\n",
        "# Applying the vectorizer\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "print(\"Number of features: %d\" % X_tfidf.get_shape()[1])\n",
        "\n",
        "# Splitting into train and test sets\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.40, random_state=0)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features: 70088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vodLiQ_t8KRZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "metadata": {
        "id": "xqymOc3f8JpY",
        "colab_type": "code",
        "outputId": "c7dd04b5-4252-461b-ac68-604ddfc8ac64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Our SVD data reducer.  We are going to reduce the feature space from upwards of 70k to 1500 \n",
        "#Ideally we would try to capture no less than 90% variance in the dataset, but due to computational limitations, we'll have to sacrifice variance\n",
        "svd= TruncatedSVD(800)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "# Run SVD on the training data, then project the training data.\n",
        "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
        "\n",
        "variance_explained=svd.explained_variance_ratio_\n",
        "total_variance = variance_explained.sum()\n",
        "\n",
        "print(\"Percent variance captured by all components:\",total_variance*100)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent variance captured by all components: 54.67715297552219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PzGvvRWB8N8R",
        "colab_type": "code",
        "outputId": "7fbd2545-3893-42e0-e704-eb43d223dfef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "cell_type": "code",
      "source": [
        "tfidf_df = pd.DataFrame(data=X_train_lsa)\n",
        "tfidf_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>790</th>\n",
              "      <th>791</th>\n",
              "      <th>792</th>\n",
              "      <th>793</th>\n",
              "      <th>794</th>\n",
              "      <th>795</th>\n",
              "      <th>796</th>\n",
              "      <th>797</th>\n",
              "      <th>798</th>\n",
              "      <th>799</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.166290</td>\n",
              "      <td>-0.021188</td>\n",
              "      <td>-0.010111</td>\n",
              "      <td>-0.094210</td>\n",
              "      <td>-0.045696</td>\n",
              "      <td>0.061653</td>\n",
              "      <td>-0.114660</td>\n",
              "      <td>0.103962</td>\n",
              "      <td>-0.008434</td>\n",
              "      <td>-0.174525</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.013930</td>\n",
              "      <td>-0.002307</td>\n",
              "      <td>-0.011600</td>\n",
              "      <td>0.016014</td>\n",
              "      <td>0.001632</td>\n",
              "      <td>-0.015421</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>-0.019010</td>\n",
              "      <td>0.002874</td>\n",
              "      <td>-0.009981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.228973</td>\n",
              "      <td>-0.038255</td>\n",
              "      <td>-0.009373</td>\n",
              "      <td>-0.123023</td>\n",
              "      <td>-0.066915</td>\n",
              "      <td>0.071071</td>\n",
              "      <td>-0.017997</td>\n",
              "      <td>0.116486</td>\n",
              "      <td>-0.027587</td>\n",
              "      <td>-0.030227</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005575</td>\n",
              "      <td>-0.029080</td>\n",
              "      <td>-0.016430</td>\n",
              "      <td>0.023663</td>\n",
              "      <td>-0.034566</td>\n",
              "      <td>-0.012798</td>\n",
              "      <td>-0.022733</td>\n",
              "      <td>-0.007202</td>\n",
              "      <td>-0.041175</td>\n",
              "      <td>0.021571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.389634</td>\n",
              "      <td>-0.176921</td>\n",
              "      <td>-0.059391</td>\n",
              "      <td>0.116729</td>\n",
              "      <td>-0.036942</td>\n",
              "      <td>0.047201</td>\n",
              "      <td>-0.221017</td>\n",
              "      <td>-0.113472</td>\n",
              "      <td>0.093444</td>\n",
              "      <td>0.011931</td>\n",
              "      <td>...</td>\n",
              "      <td>0.014608</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>-0.022467</td>\n",
              "      <td>-0.010976</td>\n",
              "      <td>0.012148</td>\n",
              "      <td>0.011027</td>\n",
              "      <td>-0.008078</td>\n",
              "      <td>0.014339</td>\n",
              "      <td>-0.015500</td>\n",
              "      <td>0.019188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.410515</td>\n",
              "      <td>-0.042928</td>\n",
              "      <td>0.005748</td>\n",
              "      <td>-0.106477</td>\n",
              "      <td>-0.013753</td>\n",
              "      <td>-0.065522</td>\n",
              "      <td>-0.127946</td>\n",
              "      <td>-0.050769</td>\n",
              "      <td>-0.102370</td>\n",
              "      <td>-0.083115</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022135</td>\n",
              "      <td>-0.004636</td>\n",
              "      <td>0.003175</td>\n",
              "      <td>0.024031</td>\n",
              "      <td>0.025389</td>\n",
              "      <td>0.010698</td>\n",
              "      <td>-0.027674</td>\n",
              "      <td>0.025074</td>\n",
              "      <td>0.016551</td>\n",
              "      <td>0.006084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.224018</td>\n",
              "      <td>-0.066809</td>\n",
              "      <td>-0.016698</td>\n",
              "      <td>-0.060471</td>\n",
              "      <td>-0.083885</td>\n",
              "      <td>0.115368</td>\n",
              "      <td>-0.018148</td>\n",
              "      <td>-0.007591</td>\n",
              "      <td>-0.097602</td>\n",
              "      <td>0.023154</td>\n",
              "      <td>...</td>\n",
              "      <td>0.025484</td>\n",
              "      <td>-0.063162</td>\n",
              "      <td>0.017254</td>\n",
              "      <td>0.039462</td>\n",
              "      <td>-0.048475</td>\n",
              "      <td>-0.005381</td>\n",
              "      <td>0.033367</td>\n",
              "      <td>-0.083228</td>\n",
              "      <td>0.014691</td>\n",
              "      <td>0.007522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 800 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  0.166290 -0.021188 -0.010111 -0.094210 -0.045696  0.061653 -0.114660   \n",
              "1  0.228973 -0.038255 -0.009373 -0.123023 -0.066915  0.071071 -0.017997   \n",
              "2  0.389634 -0.176921 -0.059391  0.116729 -0.036942  0.047201 -0.221017   \n",
              "3  0.410515 -0.042928  0.005748 -0.106477 -0.013753 -0.065522 -0.127946   \n",
              "4  0.224018 -0.066809 -0.016698 -0.060471 -0.083885  0.115368 -0.018148   \n",
              "\n",
              "        7         8         9      ...          790       791       792  \\\n",
              "0  0.103962 -0.008434 -0.174525    ...    -0.013930 -0.002307 -0.011600   \n",
              "1  0.116486 -0.027587 -0.030227    ...    -0.005575 -0.029080 -0.016430   \n",
              "2 -0.113472  0.093444  0.011931    ...     0.014608  0.001000 -0.022467   \n",
              "3 -0.050769 -0.102370 -0.083115    ...     0.022135 -0.004636  0.003175   \n",
              "4 -0.007591 -0.097602  0.023154    ...     0.025484 -0.063162  0.017254   \n",
              "\n",
              "        793       794       795       796       797       798       799  \n",
              "0  0.016014  0.001632 -0.015421  0.001264 -0.019010  0.002874 -0.009981  \n",
              "1  0.023663 -0.034566 -0.012798 -0.022733 -0.007202 -0.041175  0.021571  \n",
              "2 -0.010976  0.012148  0.011027 -0.008078  0.014339 -0.015500  0.019188  \n",
              "3  0.024031  0.025389  0.010698 -0.027674  0.025074  0.016551  0.006084  \n",
              "4  0.039462 -0.048475 -0.005381  0.033367 -0.083228  0.014691  0.007522  \n",
              "\n",
              "[5 rows x 800 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "j4zcSGDC8PcS",
        "colab_type": "code",
        "outputId": "f408be0a-a07e-4838-e9f9-47a0038876f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        }
      },
      "cell_type": "code",
      "source": [
        "new_df = pd.concat([tfidf_df, feature_df], ignore_index=False, axis=1)\n",
        "new_df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>796</th>\n",
              "      <th>797</th>\n",
              "      <th>798</th>\n",
              "      <th>799</th>\n",
              "      <th>word_count</th>\n",
              "      <th>exclamation_marks</th>\n",
              "      <th>periods</th>\n",
              "      <th>question_marks</th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.166290</td>\n",
              "      <td>-0.021188</td>\n",
              "      <td>-0.010111</td>\n",
              "      <td>-0.094210</td>\n",
              "      <td>-0.045696</td>\n",
              "      <td>0.061653</td>\n",
              "      <td>-0.114660</td>\n",
              "      <td>0.103962</td>\n",
              "      <td>-0.008434</td>\n",
              "      <td>-0.174525</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>-0.019010</td>\n",
              "      <td>0.002874</td>\n",
              "      <td>-0.009981</td>\n",
              "      <td>64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.228973</td>\n",
              "      <td>-0.038255</td>\n",
              "      <td>-0.009373</td>\n",
              "      <td>-0.123023</td>\n",
              "      <td>-0.066915</td>\n",
              "      <td>0.071071</td>\n",
              "      <td>-0.017997</td>\n",
              "      <td>0.116486</td>\n",
              "      <td>-0.027587</td>\n",
              "      <td>-0.030227</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.022733</td>\n",
              "      <td>-0.007202</td>\n",
              "      <td>-0.041175</td>\n",
              "      <td>0.021571</td>\n",
              "      <td>204</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.389634</td>\n",
              "      <td>-0.176921</td>\n",
              "      <td>-0.059391</td>\n",
              "      <td>0.116729</td>\n",
              "      <td>-0.036942</td>\n",
              "      <td>0.047201</td>\n",
              "      <td>-0.221017</td>\n",
              "      <td>-0.113472</td>\n",
              "      <td>0.093444</td>\n",
              "      <td>0.011931</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008078</td>\n",
              "      <td>0.014339</td>\n",
              "      <td>-0.015500</td>\n",
              "      <td>0.019188</td>\n",
              "      <td>175</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.410515</td>\n",
              "      <td>-0.042928</td>\n",
              "      <td>0.005748</td>\n",
              "      <td>-0.106477</td>\n",
              "      <td>-0.013753</td>\n",
              "      <td>-0.065522</td>\n",
              "      <td>-0.127946</td>\n",
              "      <td>-0.050769</td>\n",
              "      <td>-0.102370</td>\n",
              "      <td>-0.083115</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.027674</td>\n",
              "      <td>0.025074</td>\n",
              "      <td>0.016551</td>\n",
              "      <td>0.006084</td>\n",
              "      <td>86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.224018</td>\n",
              "      <td>-0.066809</td>\n",
              "      <td>-0.016698</td>\n",
              "      <td>-0.060471</td>\n",
              "      <td>-0.083885</td>\n",
              "      <td>0.115368</td>\n",
              "      <td>-0.018148</td>\n",
              "      <td>-0.007591</td>\n",
              "      <td>-0.097602</td>\n",
              "      <td>0.023154</td>\n",
              "      <td>...</td>\n",
              "      <td>0.033367</td>\n",
              "      <td>-0.083228</td>\n",
              "      <td>0.014691</td>\n",
              "      <td>0.007522</td>\n",
              "      <td>60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>great taffy at a great price. there was a wide...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 806 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.166290 -0.021188 -0.010111 -0.094210 -0.045696  0.061653 -0.114660   \n",
              "1  0.228973 -0.038255 -0.009373 -0.123023 -0.066915  0.071071 -0.017997   \n",
              "2  0.389634 -0.176921 -0.059391  0.116729 -0.036942  0.047201 -0.221017   \n",
              "3  0.410515 -0.042928  0.005748 -0.106477 -0.013753 -0.065522 -0.127946   \n",
              "4  0.224018 -0.066809 -0.016698 -0.060471 -0.083885  0.115368 -0.018148   \n",
              "\n",
              "          7         8         9  ...         796       797       798  \\\n",
              "0  0.103962 -0.008434 -0.174525  ...    0.001264 -0.019010  0.002874   \n",
              "1  0.116486 -0.027587 -0.030227  ...   -0.022733 -0.007202 -0.041175   \n",
              "2 -0.113472  0.093444  0.011931  ...   -0.008078  0.014339 -0.015500   \n",
              "3 -0.050769 -0.102370 -0.083115  ...   -0.027674  0.025074  0.016551   \n",
              "4 -0.007591 -0.097602  0.023154  ...    0.033367 -0.083228  0.014691   \n",
              "\n",
              "        799  word_count  exclamation_marks  periods  question_marks  \\\n",
              "0 -0.009981          64                0.0      3.0             0.0   \n",
              "1  0.021571         204                0.0      5.0             0.0   \n",
              "2  0.019188         175                0.0      9.0             0.0   \n",
              "3  0.006084          86                0.0      3.0             0.0   \n",
              "4  0.007522          60                0.0      4.0             0.0   \n",
              "\n",
              "                                                Text  Score  \n",
              "0  i have bought several of the vitality canned d...    5.0  \n",
              "1  product arrived labeled as jumbo salted peanut...    1.0  \n",
              "2  this is a confection that has been around a fe...    4.0  \n",
              "3  if you are looking for the secret ingredient i...    2.0  \n",
              "4  great taffy at a great price. there was a wide...    5.0  \n",
              "\n",
              "[5 rows x 806 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "Y-K_WmYKMJav",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df = new_df.replace([np.inf, -np.inf], np.nan).dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SUrHgCSl8R6b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ]
    },
    {
      "metadata": {
        "id": "BBIUi_pc8SZc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Declare clustering variables  \n",
        "features = new_df.drop(['Score','Text'], axis=1)\n",
        "predict = new_df['Score']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nUgo8zjr9jGY",
        "colab_type": "code",
        "outputId": "5baea385-e241-42b2-94bf-b59dd484b797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Clustering algorithms need normalization\n",
        "scalar = MinMaxScaler()\n",
        "\n",
        "scaled = scalar.fit_transform(features)\n",
        "scaled_df = pd.DataFrame(scaled)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-YxWrJv991ax",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### KMeans"
      ]
    },
    {
      "metadata": {
        "id": "BNH0foCg9lYP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Let's see what the elbow curve tells us would be the optimal K value \n",
        "distortions = []\n",
        "K = range(1,10)\n",
        "for k in K:\n",
        "    kmeanModel = KMeans(n_clusters=k).fit(scaled)\n",
        "    kmeanModel.fit(scaled)\n",
        "    distortions.append(sum(np.min(cdist(scaled, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / scaled.shape[0])\n",
        "# Plot the elbow\n",
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('Elbow Method for optimal k')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4M6mUQ9l9onw",
        "colab_type": "code",
        "outputId": "f2148f01-ee5d-44c5-ec2b-c0b5cb8e7ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "cell_type": "code",
      "source": [
        "# Declare and fit clustering model using K of 5 for our 5 scores\n",
        "kmeans = KMeans(n_clusters=5, random_state=0)\n",
        "y_pred = kmeans.fit_predict(scaled)\n",
        "\n",
        "pd.crosstab(predict, y_pred)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Score</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>254</td>\n",
              "      <td>5136</td>\n",
              "      <td>558</td>\n",
              "      <td>1395</td>\n",
              "      <td>3064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>149</td>\n",
              "      <td>3001</td>\n",
              "      <td>351</td>\n",
              "      <td>807</td>\n",
              "      <td>1748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>197</td>\n",
              "      <td>4189</td>\n",
              "      <td>470</td>\n",
              "      <td>1263</td>\n",
              "      <td>2537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>399</td>\n",
              "      <td>8132</td>\n",
              "      <td>882</td>\n",
              "      <td>2186</td>\n",
              "      <td>4714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <td>1706</td>\n",
              "      <td>35064</td>\n",
              "      <td>3678</td>\n",
              "      <td>9321</td>\n",
              "      <td>20703</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0      1     2     3      4\n",
              "Score                                \n",
              "1.0     254   5136   558  1395   3064\n",
              "2.0     149   3001   351   807   1748\n",
              "3.0     197   4189   470  1263   2537\n",
              "4.0     399   8132   882  2186   4714\n",
              "5.0    1706  35064  3678  9321  20703"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "55YmIR6J9xKd",
        "colab_type": "code",
        "outputId": "94beee56-6ce4-40e6-a0de-426adbe2b036",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(predict, y_pred)))\n",
        "print('Silhouette Score: {:0.5}'.format(silhouette_score(scaled, y_pred, metric='euclidean')))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjusted Rand Score: 0.0021326\n",
            "Silhouette Score: -0.019539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QeOTZmjA95vj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Minibatch KMeans"
      ]
    },
    {
      "metadata": {
        "id": "_RG8KHlA95Mv",
        "colab_type": "code",
        "outputId": "2cedb0e0-9ed1-4c4a-a800-855643fa67e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "cell_type": "code",
      "source": [
        "# Each batch will be made up of 200 data points.\n",
        "minibatchkmeans = MiniBatchKMeans(\n",
        "    init='k-means++',\n",
        "    n_clusters=5,\n",
        "    batch_size=200)\n",
        "minibatchkmeans.fit(scaled)\n",
        "\n",
        "# Add the new predicted cluster memberships to the data frame.\n",
        "predict_mini = minibatchkmeans.predict(scaled)\n",
        "\n",
        "# Check the MiniBatch model against our earlier one.\n",
        "print('Comparing k-means and mini batch k-means solutions:')\n",
        "print(pd.crosstab(predict, predict_mini))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comparing k-means and mini batch k-means solutions:\n",
            "col_0     0      1      2      3     4\n",
            "Score                                 \n",
            "1.0     155   2526   4108   2158  1460\n",
            "2.0      93   1436   2405   1282   840\n",
            "3.0     126   2100   3395   1731  1304\n",
            "4.0     277   3884   6512   3384  2256\n",
            "5.0    1082  16969  28380  14372  9669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_PTLE5rb-CQ8",
        "colab_type": "code",
        "outputId": "2f7bd426-2a84-4da3-febe-0b2ba3d3d6ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(predict, predict_mini)))\n",
        "print('Silhouette Score: {:0.5}'.format(silhouette_score(scaled, predict_mini, metric='euclidean')))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjusted Rand Score: 0.0017331\n",
            "Silhouette Score: -0.019541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zG_f3eeyVvno",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From what we can see with these two clustering methods scores being close to zero, it appears that most of the observations lie between two clusters. "
      ]
    },
    {
      "metadata": {
        "id": "nOzTgxbk-EbL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modeling: Training Set"
      ]
    },
    {
      "metadata": {
        "id": "c1cke65g_cZ9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression: Training Set"
      ]
    },
    {
      "metadata": {
        "id": "gyyIOpnS-FTu",
        "colab_type": "code",
        "outputId": "30236ad9-bd5e-4607-eb9d-1a5590f5ff29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression(class_weight = 'balanced')\n",
        "train = lr.fit(scaled, predict)\n",
        "lr_scores = cross_val_score(lr, scaled, predict, cv=5)\n",
        "\n",
        "print('\\nCross Validation Training Scores:{:.5f}(+/- {:.3f})'.format(lr_scores.mean(), lr_scores.std()*2))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross Validation Training Scores:0.58388(+/- 0.006)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WX3E-tOR_EUl",
        "colab_type": "code",
        "outputId": "e97c3c78-262f-4b86-c752-0a800f402100",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "sk_lr_report = classification_report(\n",
        "    digits=6,\n",
        "    y_true=predict, \n",
        "    y_pred=lr.predict(scaled))\n",
        "print('\\nClasification report:\\n', sk_lr_report)\n",
        "\n",
        "sk_lr_report2 = confusion_matrix(y_true=predict, y_pred=lr.predict(scaled))\n",
        "\n",
        "print('\\nConfusion Matrix:\\n',sk_lr_report2)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Clasification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0   0.227677  0.056597  0.090657     10407\n",
            "         2.0   0.136297  0.076948  0.098364      6056\n",
            "         3.0   0.184326  0.072551  0.104120      8656\n",
            "         4.0   0.256564  0.034145  0.060268     16313\n",
            "         5.0   0.650638  0.926212  0.764345     70472\n",
            "\n",
            "   micro avg   0.603303  0.603303  0.603303    111904\n",
            "   macro avg   0.291100  0.233290  0.223551    111904\n",
            "weighted avg   0.489951  0.603303  0.511943    111904\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  589   408   423   238  8749]\n",
            " [  207   466   241   155  4987]\n",
            " [  282   355   628   241  7150]\n",
            " [  392   606   596   557 14162]\n",
            " [ 1117  1584  1519   980 65272]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lrXvqmVt_e_g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Random Forest: Training Set"
      ]
    },
    {
      "metadata": {
        "id": "7DkGxIV6_GXY",
        "colab_type": "code",
        "outputId": "ee7dc26a-2a31-459b-d0ac-795ce267a734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(class_weight = 'balanced_subsample')\n",
        "train = rfc.fit(scaled, predict)\n",
        "rfc_scores = cross_val_score(rfc, scaled, predict, cv=5)\n",
        "\n",
        "print('\\nCross Validation Training Scores:{:.5f}(+/- {:.3f})'.format(rfc_scores.mean(), rfc_scores.std()*2))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross Validation Training Scores:0.60162(+/- 0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aRAVk_bY_Itl",
        "colab_type": "code",
        "outputId": "d74a6e44-a9c7-4c72-8dab-e7ac9372d6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "cell_type": "code",
      "source": [
        "sk_rfc_report = classification_report(\n",
        "    digits=6,\n",
        "    y_true=predict, \n",
        "    y_pred=rfc.predict(scaled))\n",
        "print('\\nClasification report:\\n', sk_rfc_report)\n",
        "\n",
        "sk_rfc_report2 = confusion_matrix(y_true=predict, y_pred=rfc.predict(scaled))\n",
        "\n",
        "print('\\nConfusion Matrix:\\n',sk_rfc_report2)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Clasification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0   0.998399  0.958874  0.978237     10407\n",
            "         2.0   0.999309  0.955416  0.976870      6056\n",
            "         3.0   0.998441  0.961876  0.979818      8656\n",
            "         4.0   0.997658  0.966223  0.981689     16313\n",
            "         5.0   0.978633  0.999588  0.989000     70472\n",
            "\n",
            "   micro avg   0.985631  0.985631  0.985631    111904\n",
            "   macro avg   0.994488  0.968396  0.981123    111904\n",
            "weighted avg   0.985896  0.985631  0.985567    111904\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 9979     0     2     4   422]\n",
            " [    8  5786     0     4   258]\n",
            " [    2     1  8326     7   320]\n",
            " [    5     2     6 15762   538]\n",
            " [    1     1     5    22 70443]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_eFpsEyk_mwQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modeling: Test Set"
      ]
    },
    {
      "metadata": {
        "id": "Wx1qXVLc_s_2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll need to process the test group as we did the training set above."
      ]
    },
    {
      "metadata": {
        "id": "i4Mmu15k_qA_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Total word count \n",
        "feature_df_test = pd.DataFrame()\n",
        "feature_df_test['word_count'] = [len(x.split()) for x in X_test.tolist()]\n",
        "\n",
        "# Count of punctuations \n",
        "feature_df_test['exclamation_marks'] = X_test.str.findall(r'[!]').str.len()\n",
        "feature_df_test['periods'] = X_test.str.findall(r'[.]').str.len()\n",
        "feature_df_test['question_marks'] = X_test.str.findall(r'[?]').str.len()\n",
        "feature_df_test['Text'] = X_test\n",
        "feature_df_test['Score'] = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "008wf3up_y8M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aaca876c-02ce-4928-bf89-1e54a08b4172"
      },
      "cell_type": "code",
      "source": [
        "#Our SVD data reducer.  We are going to reduce the feature space to 800 again for test.\n",
        "svd= TruncatedSVD(800)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "# Run SVD on the training data, then project the training data.\n",
        "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
        "\n",
        "variance_explained=svd.explained_variance_ratio_\n",
        "total_variance = variance_explained.sum()\n",
        "\n",
        "print(\"Percent variance captured by all components:\",total_variance*100)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent variance captured by all components: 54.881326752600955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sFQZhMix_-OB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "a0036d51-a52b-4049-d104-482550ea2d5e"
      },
      "cell_type": "code",
      "source": [
        "tfidf_df_test = pd.DataFrame(data=X_test_lsa)\n",
        "tfidf_df_test.head()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>790</th>\n",
              "      <th>791</th>\n",
              "      <th>792</th>\n",
              "      <th>793</th>\n",
              "      <th>794</th>\n",
              "      <th>795</th>\n",
              "      <th>796</th>\n",
              "      <th>797</th>\n",
              "      <th>798</th>\n",
              "      <th>799</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.181176</td>\n",
              "      <td>-0.059937</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>-0.107312</td>\n",
              "      <td>-0.099659</td>\n",
              "      <td>-0.035958</td>\n",
              "      <td>0.045923</td>\n",
              "      <td>0.106981</td>\n",
              "      <td>0.011003</td>\n",
              "      <td>-0.020440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010516</td>\n",
              "      <td>0.008997</td>\n",
              "      <td>0.017296</td>\n",
              "      <td>0.036113</td>\n",
              "      <td>0.010994</td>\n",
              "      <td>-0.009497</td>\n",
              "      <td>0.039784</td>\n",
              "      <td>0.011166</td>\n",
              "      <td>0.001259</td>\n",
              "      <td>-0.017866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.212698</td>\n",
              "      <td>-0.148848</td>\n",
              "      <td>0.003270</td>\n",
              "      <td>0.212605</td>\n",
              "      <td>-0.146578</td>\n",
              "      <td>-0.024711</td>\n",
              "      <td>-0.039786</td>\n",
              "      <td>-0.046825</td>\n",
              "      <td>-0.051085</td>\n",
              "      <td>-0.057712</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.001584</td>\n",
              "      <td>-0.010008</td>\n",
              "      <td>-0.011911</td>\n",
              "      <td>-0.004064</td>\n",
              "      <td>-0.002245</td>\n",
              "      <td>0.003945</td>\n",
              "      <td>0.004081</td>\n",
              "      <td>-0.001137</td>\n",
              "      <td>0.002342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.206577</td>\n",
              "      <td>-0.094419</td>\n",
              "      <td>-0.005918</td>\n",
              "      <td>-0.030058</td>\n",
              "      <td>0.068963</td>\n",
              "      <td>-0.037739</td>\n",
              "      <td>-0.049948</td>\n",
              "      <td>0.035435</td>\n",
              "      <td>0.066372</td>\n",
              "      <td>0.027093</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.006661</td>\n",
              "      <td>-0.009464</td>\n",
              "      <td>-0.013594</td>\n",
              "      <td>-0.030784</td>\n",
              "      <td>-0.005565</td>\n",
              "      <td>-0.009894</td>\n",
              "      <td>-0.012965</td>\n",
              "      <td>0.003320</td>\n",
              "      <td>-0.002857</td>\n",
              "      <td>0.022924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.246395</td>\n",
              "      <td>0.003102</td>\n",
              "      <td>0.008553</td>\n",
              "      <td>-0.120653</td>\n",
              "      <td>-0.153268</td>\n",
              "      <td>0.046762</td>\n",
              "      <td>-0.029316</td>\n",
              "      <td>-0.053981</td>\n",
              "      <td>-0.086738</td>\n",
              "      <td>-0.031397</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002710</td>\n",
              "      <td>0.003068</td>\n",
              "      <td>-0.007722</td>\n",
              "      <td>-0.028377</td>\n",
              "      <td>-0.013420</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>-0.025372</td>\n",
              "      <td>-0.001579</td>\n",
              "      <td>-0.016754</td>\n",
              "      <td>0.037419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.175321</td>\n",
              "      <td>-0.074497</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.047900</td>\n",
              "      <td>0.104728</td>\n",
              "      <td>0.047492</td>\n",
              "      <td>-0.051379</td>\n",
              "      <td>0.021920</td>\n",
              "      <td>-0.040469</td>\n",
              "      <td>0.001780</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017731</td>\n",
              "      <td>0.000482</td>\n",
              "      <td>-0.012648</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>-0.004339</td>\n",
              "      <td>-0.016784</td>\n",
              "      <td>-0.002012</td>\n",
              "      <td>-0.000642</td>\n",
              "      <td>0.004209</td>\n",
              "      <td>0.009211</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 800 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  0.181176 -0.059937  0.000115 -0.107312 -0.099659 -0.035958  0.045923   \n",
              "1  0.212698 -0.148848  0.003270  0.212605 -0.146578 -0.024711 -0.039786   \n",
              "2  0.206577 -0.094419 -0.005918 -0.030058  0.068963 -0.037739 -0.049948   \n",
              "3  0.246395  0.003102  0.008553 -0.120653 -0.153268  0.046762 -0.029316   \n",
              "4  0.175321 -0.074497  0.008334  0.047900  0.104728  0.047492 -0.051379   \n",
              "\n",
              "        7         8         9      ...          790       791       792  \\\n",
              "0  0.106981  0.011003 -0.020440    ...     0.010516  0.008997  0.017296   \n",
              "1 -0.046825 -0.051085 -0.057712    ...     0.001517  0.001584 -0.010008   \n",
              "2  0.035435  0.066372  0.027093    ...    -0.006661 -0.009464 -0.013594   \n",
              "3 -0.053981 -0.086738 -0.031397    ...    -0.002710  0.003068 -0.007722   \n",
              "4  0.021920 -0.040469  0.001780    ...     0.017731  0.000482 -0.012648   \n",
              "\n",
              "        793       794       795       796       797       798       799  \n",
              "0  0.036113  0.010994 -0.009497  0.039784  0.011166  0.001259 -0.017866  \n",
              "1 -0.011911 -0.004064 -0.002245  0.003945  0.004081 -0.001137  0.002342  \n",
              "2 -0.030784 -0.005565 -0.009894 -0.012965  0.003320 -0.002857  0.022924  \n",
              "3 -0.028377 -0.013420  0.000222 -0.025372 -0.001579 -0.016754  0.037419  \n",
              "4  0.001059 -0.004339 -0.016784 -0.002012 -0.000642  0.004209  0.009211  \n",
              "\n",
              "[5 rows x 800 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "VhxteJGHNpia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "12c62c4a-01ac-41ba-bf1a-635bafd406f2"
      },
      "cell_type": "code",
      "source": [
        "new_df = pd.concat([tfidf_df_test, feature_df_test], ignore_index=False, axis=1)\n",
        "new_df.head()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>796</th>\n",
              "      <th>797</th>\n",
              "      <th>798</th>\n",
              "      <th>799</th>\n",
              "      <th>word_count</th>\n",
              "      <th>exclamation_marks</th>\n",
              "      <th>periods</th>\n",
              "      <th>question_marks</th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.181176</td>\n",
              "      <td>-0.059937</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>-0.107312</td>\n",
              "      <td>-0.099659</td>\n",
              "      <td>-0.035958</td>\n",
              "      <td>0.045923</td>\n",
              "      <td>0.106981</td>\n",
              "      <td>0.011003</td>\n",
              "      <td>-0.020440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039784</td>\n",
              "      <td>0.011166</td>\n",
              "      <td>0.001259</td>\n",
              "      <td>-0.017866</td>\n",
              "      <td>65</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.212698</td>\n",
              "      <td>-0.148848</td>\n",
              "      <td>0.003270</td>\n",
              "      <td>0.212605</td>\n",
              "      <td>-0.146578</td>\n",
              "      <td>-0.024711</td>\n",
              "      <td>-0.039786</td>\n",
              "      <td>-0.046825</td>\n",
              "      <td>-0.051085</td>\n",
              "      <td>-0.057712</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003945</td>\n",
              "      <td>0.004081</td>\n",
              "      <td>-0.001137</td>\n",
              "      <td>0.002342</td>\n",
              "      <td>30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.206577</td>\n",
              "      <td>-0.094419</td>\n",
              "      <td>-0.005918</td>\n",
              "      <td>-0.030058</td>\n",
              "      <td>0.068963</td>\n",
              "      <td>-0.037739</td>\n",
              "      <td>-0.049948</td>\n",
              "      <td>0.035435</td>\n",
              "      <td>0.066372</td>\n",
              "      <td>0.027093</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.012965</td>\n",
              "      <td>0.003320</td>\n",
              "      <td>-0.002857</td>\n",
              "      <td>0.022924</td>\n",
              "      <td>58</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.246395</td>\n",
              "      <td>0.003102</td>\n",
              "      <td>0.008553</td>\n",
              "      <td>-0.120653</td>\n",
              "      <td>-0.153268</td>\n",
              "      <td>0.046762</td>\n",
              "      <td>-0.029316</td>\n",
              "      <td>-0.053981</td>\n",
              "      <td>-0.086738</td>\n",
              "      <td>-0.031397</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025372</td>\n",
              "      <td>-0.001579</td>\n",
              "      <td>-0.016754</td>\n",
              "      <td>0.037419</td>\n",
              "      <td>115</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.175321</td>\n",
              "      <td>-0.074497</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.047900</td>\n",
              "      <td>0.104728</td>\n",
              "      <td>0.047492</td>\n",
              "      <td>-0.051379</td>\n",
              "      <td>0.021920</td>\n",
              "      <td>-0.040469</td>\n",
              "      <td>0.001780</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002012</td>\n",
              "      <td>-0.000642</td>\n",
              "      <td>0.004209</td>\n",
              "      <td>0.009211</td>\n",
              "      <td>76</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 806 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.181176 -0.059937  0.000115 -0.107312 -0.099659 -0.035958  0.045923   \n",
              "1  0.212698 -0.148848  0.003270  0.212605 -0.146578 -0.024711 -0.039786   \n",
              "2  0.206577 -0.094419 -0.005918 -0.030058  0.068963 -0.037739 -0.049948   \n",
              "3  0.246395  0.003102  0.008553 -0.120653 -0.153268  0.046762 -0.029316   \n",
              "4  0.175321 -0.074497  0.008334  0.047900  0.104728  0.047492 -0.051379   \n",
              "\n",
              "          7         8         9  ...         796       797       798  \\\n",
              "0  0.106981  0.011003 -0.020440  ...    0.039784  0.011166  0.001259   \n",
              "1 -0.046825 -0.051085 -0.057712  ...    0.003945  0.004081 -0.001137   \n",
              "2  0.035435  0.066372  0.027093  ...   -0.012965  0.003320 -0.002857   \n",
              "3 -0.053981 -0.086738 -0.031397  ...   -0.025372 -0.001579 -0.016754   \n",
              "4  0.021920 -0.040469  0.001780  ...   -0.002012 -0.000642  0.004209   \n",
              "\n",
              "        799  word_count  exclamation_marks  periods  question_marks  Text  \\\n",
              "0 -0.017866          65                NaN      NaN             NaN   NaN   \n",
              "1  0.002342          30                NaN      NaN             NaN   NaN   \n",
              "2  0.022924          58                NaN      NaN             NaN   NaN   \n",
              "3  0.037419         115                NaN      NaN             NaN   NaN   \n",
              "4  0.009211          76                NaN      NaN             NaN   NaN   \n",
              "\n",
              "   Score  \n",
              "0    NaN  \n",
              "1    NaN  \n",
              "2    NaN  \n",
              "3    NaN  \n",
              "4    NaN  \n",
              "\n",
              "[5 rows x 806 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "0Q1d-m6iNvbR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df = new_df.replace([np.inf, -np.inf], np.nan).dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nZoTMaOeN43u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c25b5fcb-9475-47e2-b109-2ede82535032"
      },
      "cell_type": "code",
      "source": [
        "# Declare variables  \n",
        "test_features = new_df.drop(['Score','Text'], axis=1)\n",
        "test_predict = new_df['Score']\n",
        "\n",
        "# Normalize data \n",
        "scalar = MinMaxScaler()\n",
        "\n",
        "test_scaled = scalar.fit_transform(test_features)\n",
        "test_df = pd.DataFrame(test_scaled)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5yLHYXO3IdYp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### KMeans"
      ]
    },
    {
      "metadata": {
        "id": "7ntri8cnN-r7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "401455c3-0581-4cbc-ec3b-d703615ba1c8"
      },
      "cell_type": "code",
      "source": [
        "# Declare and fit clustering model using K of 5 for our 5 scores\n",
        "kmeans = KMeans(n_clusters=5, random_state=0)\n",
        "y_pred = kmeans.fit_predict(test_scaled)\n",
        "\n",
        "pd.crosstab(test_predict, y_pred)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Score</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>1313</td>\n",
              "      <td>202</td>\n",
              "      <td>703</td>\n",
              "      <td>2244</td>\n",
              "      <td>390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>782</td>\n",
              "      <td>141</td>\n",
              "      <td>371</td>\n",
              "      <td>1332</td>\n",
              "      <td>217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>1188</td>\n",
              "      <td>184</td>\n",
              "      <td>576</td>\n",
              "      <td>1992</td>\n",
              "      <td>319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>2151</td>\n",
              "      <td>321</td>\n",
              "      <td>1101</td>\n",
              "      <td>3618</td>\n",
              "      <td>569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <td>9361</td>\n",
              "      <td>1412</td>\n",
              "      <td>4625</td>\n",
              "      <td>15791</td>\n",
              "      <td>2574</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0     1     2      3     4\n",
              "Score                               \n",
              "1.0    1313   202   703   2244   390\n",
              "2.0     782   141   371   1332   217\n",
              "3.0    1188   184   576   1992   319\n",
              "4.0    2151   321  1101   3618   569\n",
              "5.0    9361  1412  4625  15791  2574"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "1PSsJ6jVmojg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "7bb206fc-9a84-401d-a682-bf3e0c1d40dd"
      },
      "cell_type": "code",
      "source": [
        "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(test_predict, y_pred)))\n",
        "print('Silhouette Score: {:0.5}'.format(silhouette_score(test_scaled, y_pred, metric='euclidean')))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjusted Rand Score: 0.00092246\n",
            "Silhouette Score: -0.01949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cVSudwY2Ixnj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Minibatch KMeans"
      ]
    },
    {
      "metadata": {
        "id": "KVo4LvQSmxxN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "cf234100-be47-40df-9e1c-5cf899aead93"
      },
      "cell_type": "code",
      "source": [
        "# Each batch will be made up of 200 data points.\n",
        "minibatchkmeans = MiniBatchKMeans(\n",
        "    init='k-means++',\n",
        "    n_clusters=5,\n",
        "    batch_size=200)\n",
        "minibatchkmeans.fit(test_scaled)\n",
        "\n",
        "# Add the new predicted cluster memberships to the data frame.\n",
        "predict_mini = minibatchkmeans.predict(test_scaled)\n",
        "\n",
        "# Check the MiniBatch model against our earlier one.\n",
        "print('Comparing k-means and mini batch k-means solutions:')\n",
        "print(pd.crosstab(test_predict, predict_mini))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comparing k-means and mini batch k-means solutions:\n",
            "col_0   0      1      2    3    4\n",
            "Score                            \n",
            "1.0     7   1443   3175  126  101\n",
            "2.0     2    877   1826   73   65\n",
            "3.0     9   1250   2813   86  101\n",
            "4.0    22   2326   5027  207  178\n",
            "5.0    90  10149  21944  852  728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9zVhUpHpnIS8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "df68515c-c8ba-47ce-bd9a-7060de0aaf82"
      },
      "cell_type": "code",
      "source": [
        "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(test_predict, predict_mini)))\n",
        "print('Silhouette Score: {:0.5}'.format(silhouette_score(test_scaled, predict_mini, metric='euclidean')))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjusted Rand Score: -0.00059396\n",
            "Silhouette Score: 0.0032623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hwmC7iA5Ijgi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression: Training Set"
      ]
    },
    {
      "metadata": {
        "id": "YrtXROuzoS2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "471a2a81-6c05-45da-8f70-6634d7db4f1e"
      },
      "cell_type": "code",
      "source": [
        "print('Logistic Regression Test Score: {:0.5}'.format(lr.score(test_scaled, test_predict)))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Test Score: 0.52509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YSVHm5kPuX_n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6c577c94-21e1-48a0-bc17-7f552796eeda"
      },
      "cell_type": "code",
      "source": [
        "sk_lr_report = classification_report(\n",
        "    digits=6,\n",
        "    y_true=test_predict, \n",
        "    y_pred=lr.predict(test_scaled))\n",
        "print('\\nClasification report:\\n', sk_lr_report)\n",
        "\n",
        "sk_lr_report2 = confusion_matrix(y_true=test_predict, y_pred=lr.predict(test_scaled))\n",
        "\n",
        "print('\\nConfusion Matrix:\\n',sk_lr_report2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Clasification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0   0.186644  0.022465  0.040103      4852\n",
            "         2.0   0.075032  0.223356  0.112330      2843\n",
            "         3.0   0.129056  0.125147  0.127071      4259\n",
            "         4.0   0.191348  0.014820  0.027509      7760\n",
            "         5.0   0.672259  0.790451  0.726580     33763\n",
            "\n",
            "   micro avg   0.525086  0.525086  0.525086     53477\n",
            "   macro avg   0.250868  0.235248  0.206718     53477\n",
            "weighted avg   0.483402  0.525086  0.482452     53477\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  109   941   473    59  3270]\n",
            " [   49   635   347    31  1781]\n",
            " [   55  1023   533    69  2579]\n",
            " [   90  1441   733   115  5381]\n",
            " [  281  4423  2044   327 26688]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mAS40lwJJKsc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Random Forest: Training Set"
      ]
    },
    {
      "metadata": {
        "id": "Da_a_SOEuvVa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c067f2ab-d718-46eb-e918-216b6ab108b7"
      },
      "cell_type": "code",
      "source": [
        "print('Random Forest Test Score: {:0.5}'.format(rfc.score(test_scaled, test_predict)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest Test Score: 0.59282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m63k6HGovXKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "6e871d53-ff90-416c-83a2-ef53f190daa5"
      },
      "cell_type": "code",
      "source": [
        "sk_rfc_report = classification_report(\n",
        "    digits=6,\n",
        "    y_true=test_predict, \n",
        "    y_pred=rfc.predict(test_scaled))\n",
        "print('\\nClasification report:\\n', sk_rfc_report)\n",
        "\n",
        "sk_rfc_report2 = confusion_matrix(y_true=test_predict, y_pred=rfc.predict(test_scaled))\n",
        "\n",
        "print('\\nConfusion Matrix:\\n',sk_rfc_report2)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Clasification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         1.0   0.093567  0.026381  0.041158      4852\n",
            "         2.0   0.065891  0.011959  0.020244      2843\n",
            "         3.0   0.064516  0.005166  0.009565      4259\n",
            "         4.0   0.149924  0.037887  0.060488      7760\n",
            "         5.0   0.633462  0.924799  0.751896     33763\n",
            "\n",
            "   micro avg   0.592816  0.592816  0.592816     53477\n",
            "   macro avg   0.201472  0.201238  0.176670     53477\n",
            "weighted avg   0.438826  0.592816  0.489064     53477\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[  128    49    35   178  4462]\n",
            " [   78    34    30   118  2583]\n",
            " [  116    45    22   179  3897]\n",
            " [  213    77    51   294  7125]\n",
            " [  833   311   203  1192 31224]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K4yrUIGD_yyK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "lYvRLlSEOIKD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Modeling versus Clustering?\n",
        "\n",
        "Overall, our supervised models were much better at classifying reviews than clustering. Though our models did tend to overfit on the test set. \n",
        "\n",
        "Unfortunately we weren't able to test many different clustering methods given the size of the dataset and computational limitations, and thus our Kmeans cluster was effectively the 'best' given its ability to actualy execute on the dataset.\n",
        "\n",
        "Ideally, we would be able to keep around 90% of the variance in our truncated SVD, but due to computational limitations, I was only able to perform clustering and modeling with 50% of the variance kept. Without this limitation, we would likely see an increase in our clustering  *and* modeling methods. \n",
        "\n",
        "In the future, I would like to fine tune parameters on our clusters and models, increase our variance incorporated on the tfidf vectorizer to see if we can better our overall  performances. "
      ]
    }
  ]
}