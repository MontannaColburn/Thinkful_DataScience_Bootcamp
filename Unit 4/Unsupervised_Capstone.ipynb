{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unsupervised Capstone.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FQ6idFwvVWMi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Unit 4 Unsupervised Learning Capstone Project"
      ]
    },
    {
      "metadata": {
        "id": "hvYKzDFtVodJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "tYUnfbggV0xC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ]
    },
    {
      "metadata": {
        "id": "DVkA4HZWM3jW",
        "colab_type": "code",
        "outputId": "d9f04f29-f0ca-4f76-f98b-5f11dd9bdfc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "#Basic imports \n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "#NLP imports \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Dimension Reduction \n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "#Clustering \n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "#Clustering Evaluation \n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "#Model Imports  \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "\n",
        "#Model Optimization \n",
        "from sklearn import ensemble\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.model_selection import RandomizedSearchCV \n",
        "\n",
        "#Time\n",
        "import time "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FTgUg4DmV3-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Upload, Review, and Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "7Nl07WjTUtUx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://www.dropbox.com/s/d4ye48a67tth2ae/Reviews.csv?dl=1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VlbtGJKqU4kX",
        "colab_type": "code",
        "outputId": "e202ae26-de30-4a93-86da-decada297c34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "#drop uneccessary columns \n",
        "df.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Time', 'Summary'],axis=1,inplace=True)\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Score</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Score                                               Text\n",
              "0      5  I have bought several of the Vitality canned d...\n",
              "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
              "2      4  This is a confection that has been around a fe...\n",
              "3      2  If you are looking for the secret ingredient i...\n",
              "4      5  Great taffy at a great price.  There was a wid..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "DW3sTz-CU5Su",
        "colab_type": "code",
        "outputId": "1babcf1a-bb5a-4a96-bb67-8ce1635a3770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 568454 entries, 0 to 568453\n",
            "Data columns (total 2 columns):\n",
            "Score    568454 non-null int64\n",
            "Text     568454 non-null object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 8.7+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pqkfHaTSU7JF",
        "colab_type": "code",
        "outputId": "7cbd4cdb-7d24-4273-b6ec-4ebfab008031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Score    0\n",
              "Text     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "q5EVWki7U9BU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove duplicate reviews  \n",
        "df.drop_duplicates(subset=['Score','Text'],keep='first',inplace=True)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rzvWPdAxU_qm",
        "colab_type": "code",
        "outputId": "1a55935f-56c2-4e2d-fc9c-706a6e9bf683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "df.Score.value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    250745\n",
              "4     56074\n",
              "1     36280\n",
              "3     29772\n",
              "2     20804\n",
              "Name: Score, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "1AUjWUNL0556",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Utility function to clean text.\n",
        "def text_cleaner(text):\n",
        "    \n",
        "    # Visual inspection shows spaCy does not recognize the double dash '--'.\n",
        "    \n",
        "    text = re.sub(r'--',' ',text)\n",
        "    \n",
        "    # Removes hyperlinks \n",
        "    text = re.sub(r'<a\\s+href=(?:\"([^\"]+)\"|\\'([^\\']+)\\').*?>(.*?)</a>',' ', text)\n",
        "    \n",
        "    # Get rid of extra whitespace.\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    # Lowercase text\n",
        "    text = text.lower()\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W9wtyMVE1LME",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['Clean'] = df['Text'].apply(lambda x: text_cleaner(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uj1LkgXK-ntf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create Utility function to lemmatize our text reviews limiting variations on same words\n",
        "lemma = spacy.lang.en.English()\n",
        "\n",
        "def lemma_text(text):\n",
        "    tokens = lemma(text) \n",
        "    return([token.lemma_ for token in tokens if not token.is_punct and not token.is_stop])\n",
        "\n",
        "df['lemma_text'] = df.Clean.apply(lemma_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_dGTxFnEFCwv",
        "colab_type": "code",
        "outputId": "924a833e-bcb3-4ae8-b735-1b226ef43427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Score</th>\n",
              "      <th>Text</th>\n",
              "      <th>Clean</th>\n",
              "      <th>lemma_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>[buy, vitality, can, dog, food, product, find,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>[product, arrive, label, jumbo, salt, peanut, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>[confection, century, light, pillowy, citrus, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>[look, secret, ingredient, robitussin, believe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>great taffy at a great price. there was a wide...</td>\n",
              "      <td>[great, taffy, great, price, wide, assortment,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Score                                               Text  \\\n",
              "0      5  I have bought several of the Vitality canned d...   \n",
              "1      1  Product arrived labeled as Jumbo Salted Peanut...   \n",
              "2      4  This is a confection that has been around a fe...   \n",
              "3      2  If you are looking for the secret ingredient i...   \n",
              "4      5  Great taffy at a great price.  There was a wid...   \n",
              "\n",
              "                                               Clean  \\\n",
              "0  i have bought several of the vitality canned d...   \n",
              "1  product arrived labeled as jumbo salted peanut...   \n",
              "2  this is a confection that has been around a fe...   \n",
              "3  if you are looking for the secret ingredient i...   \n",
              "4  great taffy at a great price. there was a wide...   \n",
              "\n",
              "                                          lemma_text  \n",
              "0  [buy, vitality, can, dog, food, product, find,...  \n",
              "1  [product, arrive, label, jumbo, salt, peanut, ...  \n",
              "2  [confection, century, light, pillowy, citrus, ...  \n",
              "3  [look, secret, ingredient, robitussin, believe...  \n",
              "4  [great, taffy, great, price, wide, assortment,...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "g9jAzZY45Ovv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Feature Generation "
      ]
    },
    {
      "metadata": {
        "id": "CRwaI7Xj5nAA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Identifying variables\n",
        "X = df['Clean']\n",
        "y = df['Score']\n",
        "\n",
        "# Splitting into train and test sets, reserve 25% for test \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lEZk0Ea7_nqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Total word count \n",
        "feature_df = pd.DataFrame()\n",
        "feature_df['word_count'] = [len(x.split()) for x in X_train.tolist()]\n",
        "\n",
        "# Count of punctuations \n",
        "feature_df['exclamation_marks'] = X_train.str.findall(r'[!]').str.len()\n",
        "feature_df['periods'] = X_train.str.findall(r'[.]').str.len()\n",
        "feature_df['question_marks'] = X_train.str.findall(r'[?]').str.len()\n",
        "feature_df['Text'] = X_train\n",
        "feature_df['Score'] = y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "WPGWReVNAsKk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 954
        },
        "outputId": "318132be-2e93-405c-c5cb-5e0657357351"
      },
      "cell_type": "code",
      "source": [
        "# Loop through text for word positions \n",
        "for row in X_train: \n",
        "  tokens = lemma(row)\n",
        "  noun = 0\n",
        "  verb = 0\n",
        "  adj = 0\n",
        "  proper = 0 \n",
        "  for token in tokens:\n",
        "   # Identifying each part of speech and adding to counts\n",
        "    if token.pos_ == 'NOUN':\n",
        "      noun +=1\n",
        "    elif token.pos_ == 'VERB':\n",
        "      verb +=1\n",
        "    elif token.pos_ == 'ADJ':\n",
        "      adj +=1   \n",
        "    elif token.pos_ == 'PROPN':\n",
        "      proper +=1\n",
        "    # Creating a list of all features for each sentence\n",
        "    feature_df.append([noun, verb, adj, proper])\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/api.py:87: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
            "  result = result.union(other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-6caaaeb2b55e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mproper\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Creating a list of all features for each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfeature_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   5192\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5193\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[0;32m-> 5194\u001b[0;31m                       verify_integrity=verify_integrity)\n\u001b[0m\u001b[1;32m   5195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5196\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                        copy=copy)\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    407\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5202\u001b[0m             b = make_block(\n\u001b[0;32m-> 5203\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5204\u001b[0m                 placement=placement)\n\u001b[1;32m   5205\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5338\u001b[0m             \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5339\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5340\u001b[0;31m         \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5342\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/dtypes/concat.py\u001b[0m in \u001b[0;36m_concat_compat\u001b[0;34m(to_concat, axis)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "13O1A3H7Pskb",
        "colab_type": "code",
        "outputId": "c332f302-1693-4e50-b978-2acdbb2007d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "# Let's check our new features\n",
        "feature_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_count</th>\n",
              "      <th>exclamation_marks</th>\n",
              "      <th>periods</th>\n",
              "      <th>question_marks</th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>great taffy at a great price. there was a wide...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_count  exclamation_marks  periods  question_marks  \\\n",
              "0          22                0.0      3.0             0.0   \n",
              "1          78                0.0      5.0             0.0   \n",
              "2          24                0.0      9.0             0.0   \n",
              "3          37                0.0      3.0             0.0   \n",
              "4          56                0.0      4.0             0.0   \n",
              "\n",
              "                                                Text  Score  \n",
              "0  i have bought several of the vitality canned d...    5.0  \n",
              "1  product arrived labeled as jumbo salted peanut...    1.0  \n",
              "2  this is a confection that has been around a fe...    4.0  \n",
              "3  if you are looking for the secret ingredient i...    2.0  \n",
              "4  great taffy at a great price. there was a wide...    5.0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "HndOhV5sJhDy",
        "colab_type": "code",
        "outputId": "046bba66-c86a-4694-902e-f84a57aa0f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialize vectorizer \n",
        "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
        "                             min_df=2, # only use words that appear at least twice\n",
        "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
        "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
        "                             smooth_idf=True, #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
        "                             tokenizer=lemma_text)\n",
        "\n",
        "\n",
        "# Applying the vectorizer\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "print(\"Number of features: %d\" % X_tfidf.get_shape()[1])\n",
        "\n",
        "# Splitting into train and test sets\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Reshape vectorizer to readable content\n",
        "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
        "\n",
        "# Number of paragraphs\n",
        "n = X_train_tfidf_csr.shape[0]\n",
        "\n",
        "# A list of dictionaries, one per paragraph\n",
        "tfidf_bypara = [{} for _ in range(0,n)]\n",
        "\n",
        "# List of features\n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "# For each paragraph, lists the feature words and their tf-idf scores\n",
        "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
        "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
        "\n",
        "# Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
        "print('Original sentence:', X_train[5])\n",
        "print('Tf_idf vector:', tfidf_bypara[5])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features: 70088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-5842baf66506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original sentence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tf_idf vector:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_bypara\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2560\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2561\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 5"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IGIxdbyHR4jA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "metadata": {
        "id": "Ut5tIiyW-jpr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1146
        },
        "outputId": "cc7f8c7f-6fd0-41f9-81b3-7104a123f1a8"
      },
      "cell_type": "code",
      "source": [
        "#Our SVD data reducer.  We are going to reduce the feature space from upwards of 70k to 150.\n",
        "svd= TruncatedSVD(150)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "# Run SVD on the training data, then project the training data.\n",
        "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
        "\n",
        "variance_explained=svd.explained_variance_ratio_\n",
        "total_variance = variance_explained.sum()\n",
        "\n",
        "print(\"Percent variance captured by all components:\",total_variance*100)\n",
        "\n",
        "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
        "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
        "for i in range(5):\n",
        "    print('Component {}:'.format(i))\n",
        "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent variance captured by all components: 25.6517784562051\n",
            "Component 0:\n",
            "Clean\n",
            "skor bars were a gift to my son; they are his favorite. hard to find on the open market place. no need to say he loved the gift!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0.699443\n",
            "this is the same product sold in the 'brick and mortar' stores for much more $$$$. i keep it on auto-buy. it tastes and works the same and the customer service has been stellar!!!! thanks!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.684279\n",
            "it took some time, but i finally got my two cats to like newman's own organics advanced cat dry formula. my red tabby male had a penchant for upchucking his food and this has put a stop to that. the pellets are small and hard and they seem to love crunching on them. i also mix newman's own organics advanced cat dry formula with small amounts of other dry cat food as treats for them. this is premium cat food and i love the packaging's very effective zip lock and the graphics to. i highly recommend newman's own organics.                                                                                                                                                                                                                                                                                     0.669123\n",
            "having a keurig b60 brewer but finding the k-cup hot cocoa rather expensive and not having found a k-cup brand with a good taste i thought i would give the swiss miss hot cocoa mix, diet with calcium, 8-count envelopes (pack of 6) a try. i was pleasantly surprised when i gave it a try. i used the keurig on the 6 oz setting and filled my cup with the 6 oz of hot water...poured sometimes 1 envelope and other times 2 envelopes of the swiss miss in the cup depending on my wanting a more chocolate taste or not. stir the cocoa with a whisk to break up the clumps then put the cup back in the keurig and add another 6 oz of hot water. to me the taste has been better than any k-cup cocoa mix i had tried. i love my hot cocoa during these winter evenings and sometimes during those cold winter days.    0.667195\n",
            "this really is a great candy, and very hard to find in stores.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   0.665312\n",
            "i think teatree oil is a miracle product and i could give this five stars just for having it as an ingredient!!! it's great for dandruff, itchy scalp....skin rashes, etc etc. look into it!! but it has a powerful scent. this shampoo smells great with a whisp of teatree oil. it does control dandruff but in my opinion generic head and soulders at walmart does a little better, for cheaper. yet...overall i'd get this first because of the teatree!!                                                                                                                                                                                                                                                                                                                                                                   0.663100\n",
            "exactly what i was looking for. nice container, i was very happy with the packaging. if you don't yet cook with ghee, this is a fantastic start.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 0.662969\n",
            "to me, this just tasted like apple juice with a slight strawberry aftertaste. it is also a lot of calories for 8 oz. no more than plain apple juice, so if you normally include juice in your daily diet, this could be a fun alternative. usually i don't drink juice, i go for calorie free beverages (either water, diet soda or flavored carbonated water) to save on calories. though this drink is marketed as \"healthy\" because it doesn't have sugar or artificial sweetener, it's still got plenty of calories for limited nutritional value.                                                                                                                                                                                                                                                                           0.659810\n",
            "this is one of my hubby's favorite candies. we have trouble finding them in our city. we used to resort to buying bags of halloween candy that would have some mixed in it. last year, we couldn't even find those. so, i did an online search, found them, purchased them & my hubby's eating them. they arrived fresh, and tasty. so very, very glad i found these online.<br /><br />in case you're wondering, sugar daddy lollipops are milk caramel. they come in two sizes; this is the smaller size & approx. 1\"x2\"x1/4\" not including the stick. if you chew on them they will get stuck in your teeth.                                                                                                                                                                                                                  0.658641\n",
            "i sent my husband last year for christmas. he loved it. this is a great gift for a special occassion?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.654365\n",
            "Name: 0, dtype: float64\n",
            "Component 1:\n",
            "Clean\n",
            "when i eliminated gluten from my diet a few years ago, i cleaned out my cupboards of anything that contained that menacing little protein. i bought (and mixed) countless flours and grains trying to find a good substitute for all-purpose flour. every flour and blend that i tried produced baked goods that were either tough and rubbery or mealy and crumbly. no amount of recipe-tweaking produced anything that i could happily feed to guests. enter pamela's ultimate baking & pancake mix. now i can serve fluffy and delicious pancakes, bake awesome quick breads, and create yummy coatings for pan-frying. it's been a while since i ordered so the packaging may have changed since then, but my bags are printed with nine different recipes, which gave me a good place to start.                                                             0.759204\n",
            "after living in germany i much prefer tchibo over other coffee brands and purchase it when traveling abroad. i only wish they would start marketing the product at stores in the u.s.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.737788\n",
            "i make a banana pancake with pamela's pancake mix and everyone loves it..it is fluffy, tasty and a perfect pancake really...even if you don't have to eat gf products, you will love it..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.734816\n",
            "bigelow is know for their quality teas. this one is no exception.<br />spicy, festive and holiday-evoking in both taste and aroma. it is also a gluten free product for those of use who are celiac. well done!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.729073\n",
            "i wanted flax and, boy, did i ever get it! the reviewer who said only one 40-ounce canister is included in this offer was wrong. (maybe he should inquire further about his order.) i received two 40-ounce canisters, and that is a lot of flax! included inside each canister is a 2-ounce measurer (like you use with coffee), which is very convenient. also, i was pleased to read that because this is a cold-milled flax. it does not require refrigeration, unless you want to. i am enjoying it mixed in my applesauce (gives it a thicker texture that i like) and sprinkled on my peanut butter on toast (gives the peanut butter a nutty texture). this is an introduction to flax for me (and i guess i will be a pro at using it by the time i have used 80 ounces!). i am glad i chose this particular product as my first experience with it.    0.718251\n",
            "our family enjoys many of davidson's bulk herbal teas and they are all high quality and a great bargain compared to tea bags. the green rooibos is reminiscent of green or oolong tea without the caffeine.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0.717462\n",
            "i bought this tea because my job was stressing me out and causing a lot of anxiety, indigestion, etc. and i use and love the other flavors of yogi tea. unfortunately, i can't handle the taste of this one. it's just too strong - almost burns your tongue because it is so spicy. i normally like ginger but this is too much.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0.715629\n",
            "bit expensive but worth it<br />first tried it in new zeeland and glad it is available, because we could not bring it through customs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.714791\n",
            "i tried these because i can't find no salt/reduced salt in my area. these are so good. it amazes me that they have 50 percent less salt but still taste good. after eating these, its impossible to eat full salt chips again. just wish the bags weren't half filled.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           0.709413\n",
            "i've tried using a toaster/microwave/confection oven/stove to make these things myself, but i wasn't able to. i even tried barbecuing but was unable to get the chips to \"pop\". i verified my methods with a local grocery store clerk named mat (his name has one \"t\", which seemed a little strange to me), but to no avail.<br /><br />i contacted customer support asking for instructions but they acted aloof and confused about their own product! i don't know why they couldn't just mail, or at least email me some popping instructions.<br /><br />luckily i think even raw/unpopped they taste pretty good. people look at them funny at first, but after they try a few they basically devour the whole bag, completely unpopped...                                                                                                                0.707414\n",
            "Name: 1, dtype: float64\n",
            "Component 2:\n",
            "Clean\n",
            "first the good - it tastes as good as orville's (if you like orville). now the bad - eating lady finger is like eating popcorn crumbs. if you are looking for popcorn you can eat without a spoon, try something else. the popped corn is only slightly larger than the kernels.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.717509\n",
            "i buy a 12-pack at a time, and give half away. almost everyone i have given the sauce to love it for the same reasons i do - it's much more flavorful than hot. the last person i gave some to called me after a few days and said he was using it all of the time, it was so good. it has a mild heat, but it adds enormously to the flavor of bowls of soup or gumbo, ribs, hamburgers, jambalaya, stew. and it's inexpensive. but be a comparative shopper, as i have found it as high as $5 a bottle on-line. shouldn't be more than $2 per bottle, with shipping. i forgive chef paul for \"blackening\" creole/cajun food just for this \"magic\" pepper sauce. and, yes, i am from south louisiana.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         0.706469\n",
            "i use these pods with my senseo, and the coffee is delicious! i had never even heard of senseo until i went to barcelona, and my flat had one. as soon as i got home, i purchased one from amazon, and then began experimenting with different brands of coffee pods. without doubt, wolfgang puck's is the best so far, and caramel cream is my favorite flavor.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.704177\n",
            "this was a christmas gift for my grandparents since i live out of state. i wanted to get them something that would be unique from texas. they did not say they had any problems, and they said that they loved it!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.697860\n",
            "these are the first finger snacks i gave my 8-month old, whom was already on solids at 5 months though. i found them originally at my local target. he absolutely loves them. they melt in your mouth. i've had nieces and nephews that love to snack on these too whenever they come to our house. they range from 3 to 8 years old. i like how it's just organic yogurt, and how they're perfect for little fingers. i only wished that they'd sell these in a variety pack for all the flavors. the banana mango is my son's favorite.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0.692659\n",
            "add a few tablespoons to pancake batter with a few drops of banana extract and you have found manna from heaven. if you're counting calories, dip apples in the pb2 - yummy! add enough to taste for hot chocolate beverage. elvis would have loved it!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.692152\n",
            "light and rich at the same time, these are the most amazing thing i have ever had out of a gift basket! 3 months later i'm still craving one!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.690369\n",
            "this dog food is awesome. it smells so good and it could easily be consumed by humans! speaking of... funny story... i told my husband to smell the prepared dog food i had made and he thought it smelled wonderful. so good that he took a big spoonful of it and ate it! then i told him it was dog food... :- my dogs love this stuff, but a little goes a long way. i didn't want my dogs to get a stomach ache when we first switched so i put a few spoonfuls in with their regular dog food. that seemed to work well and prolongs the last of both. the shipping was fast and the service was courteous. would order from again.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0.690338\n",
            "we started buying 100% meat diets on our vet's recommendation for getting our big fatty to lose weight on a \"catkins\" diet. cats are obligate carnivores, so feeding them other foods that contain rice, or lots of vegetables, really isn't in line with how their bodies have evolved. the vet recommended before grain and wysong, but my cats very clearly told me they like before grain much better. they lap it up, have become a healthier weight (the fat one lost, the small one gained), and have had no tummy problems ever since they switched off friskies and onto this!<br /><br />my understanding is that cats do need a few vitamins, since they do get some semi-digested vegetables from the guts of their prey. we mix in some wysong \"call of the wild\" into the meat.<br /><br />they love the taste of the beef, but it seems to irritate their tummies. i had a lot of yuckiness to clean up after they eat this, so i don't buy it anymore. my understanding is that cats naturally eat small fowl and rodents, so quail is actually the closest to what they actually eat in the wild. i have trouble imagining a kitty cat taking down a big ol' cow, so it makes sense that it didn't go over well with them!    0.688703\n",
            "the tea bags were awesome! i have been to ireland more times than i can count and i love these tea bags. i always miss them when i come home so its nice to know that i can just come on amazon and purchase them here whenever i have a craving. thanks so much!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.687268\n",
            "Name: 2, dtype: float64\n",
            "Component 3:\n",
            "Clean\n",
            "this purchase disappointed me for two reasons.<br /><br />first, i had never before tried the redskin version of feridies virginia peanuts, and i found them to be more like regular peanuts (like planters) in both size and texture than the large, super-crunchy virginia variety i was expecting.<br /><br />second, the peanuts in the cans were a bit on the stale side. unfortunately, this is a frequent problem with nut cans that are sealed with those flimsy foil tops, but this was the first time i had encountered any degree of staleness with feridies products.<br /><br />when i ordered these redskin peanuts, i also placed a separate order for the regular (non-redskin) feridies virginia peanuts, and they seemed perfectly fresh. my guess is that the cans of redskins had been languishing on the shelf for a bit too long. it would have been helpful to find some kind of \"sell by\" label on the cans, as the virginia peanuts from harry & david always have.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.702371\n",
            "after my doctor said no more coffee, not even decaf, i gave this a try. we have a home espresso machine and a daily latte is part of my morning routine. while this doesn't taste just like coffee, it is really good in its own way. it has a deep, rich flavor like coffee, that tea just doesn't have. i tried making tea lattes as a substitute at first, and they just didn't cut it. honestly, i would sometimes pick this over coffee. it is really good. the chocolate flavor is delicious.<br /><br />i put two tbsp of this in my portafilter and pull a shot as normal. you do need to scrub out your filter a bit afterwards and your portafilter needs to be completely dry before putting the teeccino in or it will stick together in a clump and the shot will not pull right. it is stickier than coffee. i also enjoy this as an iced latte. for an extra treat, put a tbsp or two of cocoa powder into the bottom of your glass, mix with the shot, then add milk and sweetener of choice, make a great iced mocha. i use coconut palm sugar, which when mixed with steamed milk creates a kind of marshmallowy taste. combined with the teeccino, yum!                                                                                                                                                                                                                                                                                                                                                                                                                               0.677124\n",
            "fast shipping, product was what i ordered and in great condition.iwill do business with this company again                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               0.675573\n",
            "the milca alpine milk chocolate is very good. the taste is creamy and my family absulutely loves it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     0.672410\n",
            "this is pretty good soy sauce. our meijers recently started carrying this. so i wanted to try it. it is much better than kikkoman soy sauce. i have several recipes that give soy sauce as an alternative ingredient. i must say using this in those recipes made it so much better! i also put a squirt in my ramen or udon noodles. yum!!!! sooooo gooood.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.666424\n",
            "my sister and i had looked all over to find the sunshine raisin biscuit cookies we both enjoy. found what i thought was the same on amazon. they aren't exactly as i remember sunshine ones but quite good. because the biscuit part is a bit harder and less sugary, at first i wasn't sure about them. however, now that i have consumed several packages, i'm not sorry i ordered and would do so again.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.661538\n",
            "when i was in my early teens, i discovered heartland granola cereal with raisin. it was the only cereal of its' type at the time. i absolutely loved the taste of the granola and molassas in this cereal, and there were always plenty of sweet and plump raisins in the box.<br /><br />i haven't seen this cereal in any of my local grocery stores in over 20 years. i have eaten many other \"natural\" cereals over the years that i enjoyed, but none as much as heartland.<br /><br />i decided to google it the other day. i was pleased to see that this cereal is still made today. i purchased a case on amazon.com and received it today. i opened it immediately and ate a huge bowl right away. the cereal tastes exactly like i remember! i still love it and am so happy that i can enjoy it once again!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.659430\n",
            "i used to buy as many nussinis as i could fit into my suitcase every time i went to austria, and would have people visiting us here bring it as well. this may be the second best chocolate in the world (pischinger mandeltorte being the best). kudos to amazon for having such a wide milka selection!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0.658650\n",
            "i saw this at target and pulled it up on amazon to check the reviews. they were great! so i bought a jar (very expensive compared to other peanut butter) and brought it home. i opened it up immediately excited to try it based on the great reviews and couldn't even eat a spoonful of it. disgusting is the only the word i can think of, it tastes like a combination of sea water (it is sooo salty) and synthetic peanut butter. all i have to say is, don't order in bulk until you try it and know you like it. yuck!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.658451\n",
            "in september 2009, my vet gave me a few of the fish-flavored chews when i picked my cat up from his teeth cleaning surgery. i wasn't sure about them, but my cat's 12, needs a special canned-food diet, & he needed anesthetized teeth cleaning every year. i figured anything to reduce the number of time he gets anesthetized was worth a try.<br /><br />she told me to break up the first chew so the cat would bite into the flavored part instead of the wierd-feeling wrapper. the cat was a bit dubious about it at first, but he was hooked within 2 treats & learned to bite through immediately. now he begs for the things and pounces with great glee when i toss one across the floor each night.<br /><br />i feed him 1 treat soon after his canned food. he takes about 10 seconds to eat it & clean up the crumbs. he just had a 7-month check up & there's not a speck of tartar on his teeth. the vet said she'd never have guessed he was 11 or ate canned food! his breath has been great, too.<br /><br />my only reservation is that i think the company may have changed the formula with the last batch i bought. they are more yellow than tan, & smell icky to me. the original chews smelled fine. my cat was ... less than enthused about the change, but he still eats them willingly. i'm not sure if i could have gotten him hooked with the new formula, but he did eventually come to accept it.<br /><br />overall score: effectiveness-10, taste/smell-4. try a small bag to see if your cat can be convinced to like the chews, then buy in bulk for savings.    0.658320\n",
            "Name: 3, dtype: float64\n",
            "Component 4:\n",
            "Clean\n",
            "this is the 3rd reorder of this popcorn. it has quickly become our family's favorite snack. the neighbor kids now complain to their parents that, \"how come we don't have popcorn like they do?\". gotta love that. super fast shipping. the portion packs are the way to go.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.639913\n",
            "we have 4 dogs ranging in size from 8 pounds to 80 pounds and all 4 love chicken soup for the dog lover's soul dry dog food for adult dog. they were eating a well known brand prior to this food. i mixed both the old food and the new food to wean them gently from the old food, so as not to upset their digestive system. they picked around the old food to eat the chicken soup for the dog lover's soul dry food! we , 2 legged and 4 legged family members, are very satisfied with the food!                                                                                                                                                                                                                                                                                                                                                                                       0.617371\n",
            "i bought this along with to make some bread and cakes.<br /><br />i wasn't too impressed with the mixer but this xanthan gum is incredible stuff for the price.<br /><br />it thickens up nicely and gives a great texture.<br /><br />i will be coming back for more.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        0.593495\n",
            "i am a bzzagent and received this coffee for free from bzzagent.i have been drinking my green mountain coffee fair trade k-cups for several days now and i am happy with them. i am so glad this coffee is fair trade but i wish the flavor was a little more to my liking - it has a little bit of a weedy after taste to it. i like how smooth it is but it has just a little back bite that would make me choose another coffee. it is not a bad drink, just not my favorite. this is definitely a medium to light roast coffee - if you want something darker, try the sumatra green mountain cups. but if you like a more mild coffee that doesn't need much cream added to it, this is the coffee for you.                                                                                                                                                                              0.576486\n",
            "i have always enjoyed noh products and especially this one. it is excellent in a variety of ways, use your imagination and enjoy.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             0.575352\n",
            "this is by far my favorite hot sauce. though not very spicy, the slightly smokey flavor gives a richness to any food, and i could eat this stuff straight from the bottle.<br /><br />love it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0.573143\n",
            "hmmm it is very bitter (according to everybody that tasted it). however i feel it is an acquired taste and with time i loved the rich feeling of almost pure cocoa in my mouth...<br />very good.. and rich in anti oxidants huh....                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          0.570499\n",
            "i have health issues that make my hair fall out after large steroid treatments and the last few years it comes back different. my hair has always been fine and thin. this time it came in frizzy and like a brillo pad. not the whole head i lose about 1/4 each time so it is a mess. i washed my hair today after swimming to really give it a challenge.<br /><br />it lathers well and has a soft pleasant fragrance. rinses out easily. i used the conditioner which i will also.<br /><br />it comes wrapped in plastic but mine leaked out into the bag so you have to kinda was the bottle, it was nice as a hand wash nothing wasted. it did not get the box wet.<br /><br />i will be buying this and the price here is less than the brand i buy at the big m store.<br />my hair is smooth and soft. not all frizz gone but this is one use and it looks at least 75% better.    0.569044\n",
            "my cats love all of the natural value cat foods. i like the quality ingredients in this brand. i prefer feeding them the chicken or turkey pate since it doesn't have a fishy smell. they really like the chicken pate and gobble this up like there's no tomorrow. another fave is the chick topped with red meat tuna. all of them (and we have a lot of cats!) love this food. our cats are healthy and beautiful, and have all been raised on natural value (even the young kittens eat this without any upset tummies). there are rarely any hairball incidents either. the price is great through amazon with free shipping. i don't order it from a reseller so can't comment on that, but the amazon orders always arrive in great shape. now if they will just put in on subscribe & save!                                                                                           0.567645\n",
            "i'm not a coffee maker so i offer a cup of cappachino to my guests, and they love this. i make at least two cups a day for my husband, a great treat. the amount and price of this product is very reasonable. i've ordered it for two others that loved the flavor and the easy make of it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.560452\n",
            "Name: 4, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aj7n8IW_wtPC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "b093b770-dfe4-4641-84dc-3a1430599f0c"
      },
      "cell_type": "code",
      "source": [
        "tfidf_df = pd.DataFrame(data=X_train_lsa)\n",
        "tfidf_df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.219359</td>\n",
              "      <td>-0.083756</td>\n",
              "      <td>-0.026249</td>\n",
              "      <td>-0.055444</td>\n",
              "      <td>0.073780</td>\n",
              "      <td>0.279669</td>\n",
              "      <td>0.130958</td>\n",
              "      <td>0.175923</td>\n",
              "      <td>0.086169</td>\n",
              "      <td>0.129219</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.034766</td>\n",
              "      <td>0.053317</td>\n",
              "      <td>0.052316</td>\n",
              "      <td>-0.026100</td>\n",
              "      <td>0.058576</td>\n",
              "      <td>0.038641</td>\n",
              "      <td>0.041538</td>\n",
              "      <td>-0.031569</td>\n",
              "      <td>0.017604</td>\n",
              "      <td>0.013154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.448277</td>\n",
              "      <td>-0.164867</td>\n",
              "      <td>-0.058756</td>\n",
              "      <td>-0.056759</td>\n",
              "      <td>-0.090661</td>\n",
              "      <td>0.178492</td>\n",
              "      <td>-0.021675</td>\n",
              "      <td>-0.084486</td>\n",
              "      <td>0.028504</td>\n",
              "      <td>0.118611</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.033832</td>\n",
              "      <td>0.054194</td>\n",
              "      <td>-0.055066</td>\n",
              "      <td>0.072192</td>\n",
              "      <td>0.013214</td>\n",
              "      <td>-0.037455</td>\n",
              "      <td>0.011321</td>\n",
              "      <td>-0.046787</td>\n",
              "      <td>-0.018572</td>\n",
              "      <td>0.012482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.512499</td>\n",
              "      <td>-0.207826</td>\n",
              "      <td>-0.046787</td>\n",
              "      <td>0.035122</td>\n",
              "      <td>0.090633</td>\n",
              "      <td>0.148265</td>\n",
              "      <td>-0.054061</td>\n",
              "      <td>0.066485</td>\n",
              "      <td>0.145101</td>\n",
              "      <td>-0.123818</td>\n",
              "      <td>...</td>\n",
              "      <td>0.083230</td>\n",
              "      <td>-0.007808</td>\n",
              "      <td>0.050956</td>\n",
              "      <td>0.013355</td>\n",
              "      <td>0.065961</td>\n",
              "      <td>0.082175</td>\n",
              "      <td>-0.025672</td>\n",
              "      <td>0.055612</td>\n",
              "      <td>0.037891</td>\n",
              "      <td>-0.125265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.249003</td>\n",
              "      <td>-0.097598</td>\n",
              "      <td>-0.012913</td>\n",
              "      <td>-0.151648</td>\n",
              "      <td>-0.017880</td>\n",
              "      <td>-0.254350</td>\n",
              "      <td>0.157916</td>\n",
              "      <td>0.159583</td>\n",
              "      <td>0.199956</td>\n",
              "      <td>-0.056765</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017753</td>\n",
              "      <td>0.048171</td>\n",
              "      <td>-0.021298</td>\n",
              "      <td>-0.018937</td>\n",
              "      <td>0.017248</td>\n",
              "      <td>-0.004006</td>\n",
              "      <td>0.039017</td>\n",
              "      <td>-0.041930</td>\n",
              "      <td>-0.032496</td>\n",
              "      <td>-0.023979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.369391</td>\n",
              "      <td>-0.103716</td>\n",
              "      <td>-0.033122</td>\n",
              "      <td>0.004461</td>\n",
              "      <td>-0.053811</td>\n",
              "      <td>-0.062152</td>\n",
              "      <td>-0.004741</td>\n",
              "      <td>0.041748</td>\n",
              "      <td>-0.062890</td>\n",
              "      <td>0.096111</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.030254</td>\n",
              "      <td>-0.037054</td>\n",
              "      <td>0.156774</td>\n",
              "      <td>-0.082027</td>\n",
              "      <td>0.023334</td>\n",
              "      <td>0.014758</td>\n",
              "      <td>0.022908</td>\n",
              "      <td>-0.108371</td>\n",
              "      <td>0.049545</td>\n",
              "      <td>-0.015205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  150 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2         3         4         5         6    \\\n",
              "0  0.219359 -0.083756 -0.026249 -0.055444  0.073780  0.279669  0.130958   \n",
              "1  0.448277 -0.164867 -0.058756 -0.056759 -0.090661  0.178492 -0.021675   \n",
              "2  0.512499 -0.207826 -0.046787  0.035122  0.090633  0.148265 -0.054061   \n",
              "3  0.249003 -0.097598 -0.012913 -0.151648 -0.017880 -0.254350  0.157916   \n",
              "4  0.369391 -0.103716 -0.033122  0.004461 -0.053811 -0.062152 -0.004741   \n",
              "\n",
              "        7         8         9      ...          140       141       142  \\\n",
              "0  0.175923  0.086169  0.129219    ...    -0.034766  0.053317  0.052316   \n",
              "1 -0.084486  0.028504  0.118611    ...    -0.033832  0.054194 -0.055066   \n",
              "2  0.066485  0.145101 -0.123818    ...     0.083230 -0.007808  0.050956   \n",
              "3  0.159583  0.199956 -0.056765    ...    -0.017753  0.048171 -0.021298   \n",
              "4  0.041748 -0.062890  0.096111    ...    -0.030254 -0.037054  0.156774   \n",
              "\n",
              "        143       144       145       146       147       148       149  \n",
              "0 -0.026100  0.058576  0.038641  0.041538 -0.031569  0.017604  0.013154  \n",
              "1  0.072192  0.013214 -0.037455  0.011321 -0.046787 -0.018572  0.012482  \n",
              "2  0.013355  0.065961  0.082175 -0.025672  0.055612  0.037891 -0.125265  \n",
              "3 -0.018937  0.017248 -0.004006  0.039017 -0.041930 -0.032496 -0.023979  \n",
              "4 -0.082027  0.023334  0.014758  0.022908 -0.108371  0.049545 -0.015205  \n",
              "\n",
              "[5 rows x 150 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "95leQGK2jHV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "outputId": "89733865-01c7-4949-a90b-ba0f9e9cf57c"
      },
      "cell_type": "code",
      "source": [
        "new_df = pd.concat([tfidf_df, feature_df], ignore_index=False, axis=1)\n",
        "new_df.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>word_count</th>\n",
              "      <th>exclamation_marks</th>\n",
              "      <th>periods</th>\n",
              "      <th>question_marks</th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.219359</td>\n",
              "      <td>-0.083756</td>\n",
              "      <td>-0.026249</td>\n",
              "      <td>-0.055444</td>\n",
              "      <td>0.073780</td>\n",
              "      <td>0.279669</td>\n",
              "      <td>0.130958</td>\n",
              "      <td>0.175923</td>\n",
              "      <td>0.086169</td>\n",
              "      <td>0.129219</td>\n",
              "      <td>...</td>\n",
              "      <td>0.041538</td>\n",
              "      <td>-0.031569</td>\n",
              "      <td>0.017604</td>\n",
              "      <td>0.013154</td>\n",
              "      <td>22</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>i have bought several of the vitality canned d...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.448277</td>\n",
              "      <td>-0.164867</td>\n",
              "      <td>-0.058756</td>\n",
              "      <td>-0.056759</td>\n",
              "      <td>-0.090661</td>\n",
              "      <td>0.178492</td>\n",
              "      <td>-0.021675</td>\n",
              "      <td>-0.084486</td>\n",
              "      <td>0.028504</td>\n",
              "      <td>0.118611</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011321</td>\n",
              "      <td>-0.046787</td>\n",
              "      <td>-0.018572</td>\n",
              "      <td>0.012482</td>\n",
              "      <td>78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.512499</td>\n",
              "      <td>-0.207826</td>\n",
              "      <td>-0.046787</td>\n",
              "      <td>0.035122</td>\n",
              "      <td>0.090633</td>\n",
              "      <td>0.148265</td>\n",
              "      <td>-0.054061</td>\n",
              "      <td>0.066485</td>\n",
              "      <td>0.145101</td>\n",
              "      <td>-0.123818</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025672</td>\n",
              "      <td>0.055612</td>\n",
              "      <td>0.037891</td>\n",
              "      <td>-0.125265</td>\n",
              "      <td>24</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>this is a confection that has been around a fe...</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.249003</td>\n",
              "      <td>-0.097598</td>\n",
              "      <td>-0.012913</td>\n",
              "      <td>-0.151648</td>\n",
              "      <td>-0.017880</td>\n",
              "      <td>-0.254350</td>\n",
              "      <td>0.157916</td>\n",
              "      <td>0.159583</td>\n",
              "      <td>0.199956</td>\n",
              "      <td>-0.056765</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039017</td>\n",
              "      <td>-0.041930</td>\n",
              "      <td>-0.032496</td>\n",
              "      <td>-0.023979</td>\n",
              "      <td>37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>if you are looking for the secret ingredient i...</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.369391</td>\n",
              "      <td>-0.103716</td>\n",
              "      <td>-0.033122</td>\n",
              "      <td>0.004461</td>\n",
              "      <td>-0.053811</td>\n",
              "      <td>-0.062152</td>\n",
              "      <td>-0.004741</td>\n",
              "      <td>0.041748</td>\n",
              "      <td>-0.062890</td>\n",
              "      <td>0.096111</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022908</td>\n",
              "      <td>-0.108371</td>\n",
              "      <td>0.049545</td>\n",
              "      <td>-0.015205</td>\n",
              "      <td>56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>great taffy at a great price. there was a wide...</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  156 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3         4         5         6  \\\n",
              "0  0.219359 -0.083756 -0.026249 -0.055444  0.073780  0.279669  0.130958   \n",
              "1  0.448277 -0.164867 -0.058756 -0.056759 -0.090661  0.178492 -0.021675   \n",
              "2  0.512499 -0.207826 -0.046787  0.035122  0.090633  0.148265 -0.054061   \n",
              "3  0.249003 -0.097598 -0.012913 -0.151648 -0.017880 -0.254350  0.157916   \n",
              "4  0.369391 -0.103716 -0.033122  0.004461 -0.053811 -0.062152 -0.004741   \n",
              "\n",
              "          7         8         9  ...         146       147       148  \\\n",
              "0  0.175923  0.086169  0.129219  ...    0.041538 -0.031569  0.017604   \n",
              "1 -0.084486  0.028504  0.118611  ...    0.011321 -0.046787 -0.018572   \n",
              "2  0.066485  0.145101 -0.123818  ...   -0.025672  0.055612  0.037891   \n",
              "3  0.159583  0.199956 -0.056765  ...    0.039017 -0.041930 -0.032496   \n",
              "4  0.041748 -0.062890  0.096111  ...    0.022908 -0.108371  0.049545   \n",
              "\n",
              "        149  word_count  exclamation_marks  periods  question_marks  \\\n",
              "0  0.013154          22                0.0      3.0             0.0   \n",
              "1  0.012482          78                0.0      5.0             0.0   \n",
              "2 -0.125265          24                0.0      9.0             0.0   \n",
              "3 -0.023979          37                0.0      3.0             0.0   \n",
              "4 -0.015205          56                0.0      4.0             0.0   \n",
              "\n",
              "                                                Text  Score  \n",
              "0  i have bought several of the vitality canned d...    5.0  \n",
              "1  product arrived labeled as jumbo salted peanut...    1.0  \n",
              "2  this is a confection that has been around a fe...    4.0  \n",
              "3  if you are looking for the secret ingredient i...    2.0  \n",
              "4  great taffy at a great price. there was a wide...    5.0  \n",
              "\n",
              "[5 rows x 156 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "Na-Bc6BaWNy-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clustering"
      ]
    },
    {
      "metadata": {
        "id": "Z4f6WuRPV8LU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert our nan values to zeros for clustering \n",
        "new_df.replace('NaN', np.nan) \n",
        "new_df.fillna(0, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eX2xZukOWPPI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### K Means"
      ]
    },
    {
      "metadata": {
        "id": "LkTIO_7hSuX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Declare clustering variables  \n",
        "cluster_features = new_df.drop(['Score','Text'], axis=1)\n",
        "cluster_predict = new_df['Score']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ceko18aWVmGo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "389db2fe-0ffc-4d8a-a463-6619303d421a"
      },
      "cell_type": "code",
      "source": [
        "# Clustering algorithms need normalization\n",
        "scalar = MinMaxScaler()\n",
        "\n",
        "cluster_scaled = scalar.fit_transform(cluster_features)\n",
        "cluster_df = pd.DataFrame(cluster_scaled)\n",
        "X_pca = PCA(2).fit_transform(cluster_df)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
            "  return self.partial_fit(X, y)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vTCLo8EETvUH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f77deac8-88d6-47f4-f1f9-083c580c08be"
      },
      "cell_type": "code",
      "source": [
        "#from sklearn.preprocessing import normalize\n",
        "# Normalize data for clustering purposes \n",
        "#cluster_norm = normalize(cluster_features) \n",
        "#print(len(cluster_norm))\n",
        "\n",
        "# Reduce components for visualizations \n",
        "#pca = PCA(n_components=2)\n",
        "#X_pca = pca.fit_transform(cluster_norm)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "295256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Un811_77PUAh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "5d91eb86-41a2-4431-d2ad-ea8b83d3d0bf"
      },
      "cell_type": "code",
      "source": [
        "# Let's see what the elbow curve tells us would be the optimal K value \n",
        "distortions = []\n",
        "K = range(1,10)\n",
        "for k in K:\n",
        "    kmeanModel = KMeans(n_clusters=k).fit(cluster_scaled)\n",
        "    kmeanModel.fit(cluster_scaled)\n",
        "    distortions.append(sum(np.min(cdist(cluster_scaled, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / cluster_scaled.shape[0])\n",
        "# Plot the elbow\n",
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('Elbow Method for optimal k')\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhUUiiIiEil/w9LrYSK5ZDvyAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "EV3a6mBnUUp1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "085ac7fe-0dd2-4805-f0b5-d2a2d55acb58"
      },
      "cell_type": "code",
      "source": [
        "# Declare and fit clustering model using K of 6 for our 6 scores\n",
        "kmeans = KMeans(n_clusters=6, random_state=0)\n",
        "y_pred = kmeans.fit_predict(cluster_scaled)\n",
        "\n",
        "pd.crosstab(cluster_predict, y_pred)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Score</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>8565</td>\n",
              "      <td>10442</td>\n",
              "      <td>10518</td>\n",
              "      <td>6749</td>\n",
              "      <td>32808</td>\n",
              "      <td>54228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>1139</td>\n",
              "      <td>1318</td>\n",
              "      <td>1345</td>\n",
              "      <td>879</td>\n",
              "      <td>4239</td>\n",
              "      <td>6954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>650</td>\n",
              "      <td>731</td>\n",
              "      <td>768</td>\n",
              "      <td>507</td>\n",
              "      <td>2475</td>\n",
              "      <td>4148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>969</td>\n",
              "      <td>1159</td>\n",
              "      <td>1171</td>\n",
              "      <td>690</td>\n",
              "      <td>3497</td>\n",
              "      <td>5875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>1697</td>\n",
              "      <td>2011</td>\n",
              "      <td>2247</td>\n",
              "      <td>1349</td>\n",
              "      <td>6724</td>\n",
              "      <td>10948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <td>7626</td>\n",
              "      <td>9186</td>\n",
              "      <td>9265</td>\n",
              "      <td>5928</td>\n",
              "      <td>29010</td>\n",
              "      <td>47441</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0     0      1      2     3      4      5\n",
              "Score                                        \n",
              "0.0    8565  10442  10518  6749  32808  54228\n",
              "1.0    1139   1318   1345   879   4239   6954\n",
              "2.0     650    731    768   507   2475   4148\n",
              "3.0     969   1159   1171   690   3497   5875\n",
              "4.0    1697   2011   2247  1349   6724  10948\n",
              "5.0    7626   9186   9265  5928  29010  47441"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "s7dl-XN4cQxf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "362a963e-8a76-4780-e4df-211acb98de45"
      },
      "cell_type": "code",
      "source": [
        "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(cluster_predict, y_pred)))\n",
        "print('Silhouette Score: {:0.5}'.format(silhouette_score(cluster_scaled, y_pred, metric='euclidean')))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adjusted Rand Score: -0.00019245\n",
            "Silhouette Score: -0.00091741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QZBuVv4UPgj7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "344213b9-35c0-4b38-cc84-c644bb4d8d32"
      },
      "cell_type": "code",
      "source": [
        "#Show scores categorized by cluster\n",
        "prediction = pd.crosstab(y_pred, cluster_predict)\n",
        "prediction.plot(kind='bar', stacked=False, figsize=[20,5])\n",
        "plt.title('Scores by Kmeans Clusters')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJsAAAFHCAYAAAAGM03uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x/nFK6NK36r6gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FF4sqeqQdQEK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bandwidth "
      ]
    },
    {
      "metadata": {
        "id": "lpvKu4wRMMAi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Due to the size of the dataset, we won't be using bandwidth as a clustering method here. "
      ]
    },
    {
      "metadata": {
        "id": "SHIrSXbwdSTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1388
        },
        "outputId": "0ebf1876-1c0c-4d3f-9a26-d298ab162278"
      },
      "cell_type": "code",
      "source": [
        "# Declare and fit the model ***Crashed session \n",
        "bandwidth = estimate_bandwidth(cluster_scaled, quantile=0.3)\n",
        "meanshift_pred = MeanShift(bandwidth=bandwidth, bin_seeding=True).fit_predict(cluster_scaled)\n",
        "\n",
        "pd.crosstab(cluster_predict, meanshift_pred)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-ad63dd09c593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbandwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_bandwidth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmeanshift_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeanShift\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbandwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_seeding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrosstab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeanshift_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cluster/mean_shift_.py\u001b[0m in \u001b[0;36mestimate_bandwidth\u001b[0;34m(X, quantile, n_samples, random_state, n_jobs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mbandwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0mbandwidth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    453\u001b[0m                 delayed_query(\n\u001b[1;32m    454\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 455\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             )\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, data, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \"\"\"\n\u001b[0;32m--> 292\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "psEiyigheMps",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get ARI and Silhouette Score\n",
        "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(cluster_predict, meanshift_pred)))\n",
        "print('Silhouette Score: {:0.5}'.format(silhouette_score(cluster_scaled, meanshift_pred, metric='euclidean')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sF1ov0XJPiDC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Show scores categorized by cluster\n",
        "prediction = pd.crosstab(meanshift_pred, cluster_predict)\n",
        "prediction.plot(kind='bar', stacked=False, figsize=[20,5])\n",
        "plt.title('Scores by Mean Shift Clusters')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BIpw6HeIWTZP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Spectral Clustering"
      ]
    },
    {
      "metadata": {
        "id": "0iqogDYkiKSq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Declare and fit the model ***Crashed session \n",
        "sc = SpectralClustering(n_clusters=6)\n",
        "spectral_pred = sc.fit_predict(cluster_scaled)\n",
        "\n",
        "pd.crosstab(cluster_predict, spectral_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0f4vP4SjC32",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get ARI and Silhouette Score \n",
        "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(cluster_predict, spectral_pred)))\n",
        "print('Silhouette Score: {:0.5}'.format(silhouette_score(cluster_scaled, spectral_pred, metric='euclidean')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3OnGHvqolwG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Show scores categorized by cluster\n",
        "prediction = pd.crosstab(spectral_pred, cluster_predict)\n",
        "prediction.plot(kind='bar', stacked=False, figsize=[20,5])\n",
        "plt.title('Scores by Spectral Clusters')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uBE5Ut3rpEcV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Affinity Propogation "
      ]
    },
    {
      "metadata": {
        "id": "obpOPM-cpDpn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Declare and fit model ***Crashed session \n",
        "ap = AffinityPropagation()\n",
        "affinity_pred = ap.fit_predict(cluster_scaled)\n",
        "\n",
        "pd.crosstab(cluster_predict, affinity_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w_f_qRrSrimG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Get ARI and Silhouette Score \n",
        "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(cluster_predict, affinity_pred)))\n",
        "print('Silhouette Score: {:0.5}'.format(silhouette_score(cluster_scaled, affinity_pred, metric='euclidean')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xc-IhCIlWu4d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modeling Training Set"
      ]
    },
    {
      "metadata": {
        "id": "tATHsQ6oQwIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "outputId": "4a55a1cb-b250-43aa-a4fd-9d3bd3286596"
      },
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression()\n",
        "train = lr.fit(cluster_scaled, cluster_predict)\n",
        "lr_scores = cross_val_score(lr, cluster_scaled, cluster_predict, cv=5)\n",
        "\n",
        "print('\\nCross Validation Training Scores:{:.5f}(+/- {:.3f})'.format(lr_scores.mean(), lr_scores.std()*2))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross Validation Training Scores:0.66478(+/- 0.003)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PBnwMQ77Ww3W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "30327b24-cf72-420f-a0d7-d0b24378cbd3"
      },
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier()\n",
        "train = rfc.fit(cluster_scaled, cluster_predict)\n",
        "rfc_scores = cross_val_score(rfc, cluster_scaled, cluster_predict, cv=5)\n",
        "\n",
        "print('\\nCross Validation Training Scores:{:.5f}(+/- {:.3f})'.format(rfc_scores.mean(), rfc_scores.std()*2))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Cross Validation Training Scores:0.75564(+/- 0.001)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cupvPdUvRMju",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gb = ensemble.GradientBoostingClassifier()\n",
        "train = gb.fit(cluster_scaled, cluster_predict)\n",
        "gb_scores = cross_val_score(gb, cluster_scaled, cluster_predict, cv=5)\n",
        "\n",
        "print('\\nCross Validation Training Scores:{:.5f}(+/- {:.3f})'.format(gb_scores.mean(), gb_scores.std()*2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XpjjdoOvippy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Modeling Test Holdout Group "
      ]
    },
    {
      "metadata": {
        "id": "0qDD-M3PqSJ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll need to process the test group as we did the training set above. "
      ]
    },
    {
      "metadata": {
        "id": "NdbNe4VMjgMl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Total word count \n",
        "feature_df_test = pd.DataFrame()\n",
        "feature_df_test['word_count'] = [len(x.split()) for x in X_test.tolist()]\n",
        "\n",
        "# Count of punctuations \n",
        "feature_df_test['exclamation_marks'] = X_test.str.findall(r'[!]').str.len()\n",
        "feature_df_test['periods'] = X_test.str.findall(r'[.]').str.len()\n",
        "feature_df_test['question_marks'] = X_test.str.findall(r'[?]').str.len()\n",
        "feature_df_test['Text'] = X_test\n",
        "feature_df_test['Score'] = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UXf3gg3MjnPu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loop through text for word positions \n",
        "for row in X_test: \n",
        "  tokens = lemma(row)\n",
        "  noun = 0\n",
        "  verb = 0\n",
        "  adj = 0\n",
        "  proper = 0 \n",
        "  for token in tokens:\n",
        "   # Identifying each part of speech and adding to counts\n",
        "    if token.pos_ == 'NOUN':\n",
        "      noun +=1\n",
        "    elif token.pos_ == 'VERB':\n",
        "      verb +=1\n",
        "    elif token.pos_ == 'ADJ':\n",
        "      adj +=1   \n",
        "    elif token.pos_ == 'PROPN':\n",
        "      proper +=1\n",
        "    # Creating a list of all features for each sentence\n",
        "    feature_df_test.append([noun, verb, adj, proper])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjBLk0Azjn0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Our SVD data reducer.  We are going to reduce the feature space to 150 again for test.\n",
        "svd= TruncatedSVD(150)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "# Run SVD on the training data, then project the training data.\n",
        "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
        "\n",
        "variance_explained=svd.explained_variance_ratio_\n",
        "total_variance = variance_explained.sum()\n",
        "\n",
        "print(\"Percent variance captured by all components:\",total_variance*100)\n",
        "\n",
        "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
        "paras_by_component=pd.DataFrame(X_test_lsa,index=X_test)\n",
        "for i in range(5):\n",
        "    print('Component {}:'.format(i))\n",
        "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nLOtXJkNkIk7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidf_df_test = pd.DataFrame(data=X_test_lsa)\n",
        "tfidf_df_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u5Yhr7WykP1x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_df_test = pd.concat([tfidf_df_test, feature_df_test], ignore_index=False, axis=1)\n",
        "new_df_test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZR0lSZ3Pkixl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Convert our nan values to zeros\n",
        "new_df_test.replace('NaN', np.nan) \n",
        "new_df_test.fillna(0, inplace=True)\n",
        "# Declare variables  \n",
        "test_features = new_df.drop(['Score','Text'], axis=1)\n",
        "test_predict = new_df['Score']\n",
        "# Normalize data \n",
        "scalar = MinMaxScaler()\n",
        "\n",
        "test_scaled = scalar.fit_transform(test_features)\n",
        "test_df = pd.DataFrame(test_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IbduJN2VrRMg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Declare and fit clustering model for test using K of 6 for our 6 scores\n",
        "kmeans = KMeans(n_clusters=6, random_state=0)\n",
        "y_pred = kmeans.fit_predict(test_scaled)\n",
        "\n",
        "pd.crosstab(test_predict, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDg4VBc0rfFX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Adjusted Rand Score: {:0.5}'.format(adjusted_rand_score(test_predict, y_pred)))\n",
        "print('Silhouette Score: {:0.5}'.format(silhouette_score(test_scaled, y_pred, metric='euclidean')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J8i3QR9crhvW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Show scores categorized by cluster\n",
        "prediction = pd.crosstab(y_pred, test_predict)\n",
        "prediction.plot(kind='bar', stacked=False, figsize=[20,5])\n",
        "plt.title('Scores by Kmeans Clusters')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GPPEknatroHd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#initialize best performing model above to test set "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZHrZdHR_WovU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "TYhAafmjWqby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wQc9c-iowUgD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Future Work"
      ]
    },
    {
      "metadata": {
        "id": "4kiJgVNdwWnA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "optimize parameters in clusters \n",
        "\n",
        "optimize parameters in models\n"
      ]
    },
    {
      "metadata": {
        "id": "Wlu3fxXmwVyA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}